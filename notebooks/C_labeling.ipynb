{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning Workshop: Content Insights 2020](assets/mlci_banner.jpg)\n",
    "\n",
    "# Machine Learning Workshop: Content Insights 2020\n",
    "\n",
    "Welcome to the workshop notebooks!  These notebooks are designed to give you a walk through the steps of creating a model, refining it with user labels, and testing it on content.  You can access the main [workshop forum page](https://INFO_SITE/forums/html/forum?id=241a0b77-7aa6-4fef-9f25-5ea351825725&ps=25), the [workshop files repo](https://INFO_SITE/communities/service/html/communityview?communityUuid=fb400868-b17c-44d8-8b63-b445d26a0be4#fullpageWidgetId=W403a0d6f86de_45aa_8b67_c52cf90fca16&folder=d8138bef-9182-4bdc-8b12-3c88158a219c), or the [symposium home page](https://software.web.DOMAIN) for additional help.\n",
    "\n",
    "The notebooks are divided into five core components: (A) setup & data, (B) model exploration, (C) labeling, (D) active labeling, (E) and deployment.  You are currently viewing the *setup & data* workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for running the workshop; we'll repeat these in the top line of each workbook.\n",
    "#   why repeat them? the backup routine only serializes .ipynb files, so others will need \n",
    "#   to be downloaded again if your compute instance restarts (a small price to pay, right?)\n",
    "\n",
    "WORKSHOP_BASE = \"https://vmlr-workshop.STORAGE\"\n",
    "# WORKSHOP_BASE = \"http://content.research.DOMAIN/projects/mlci_2020\"\n",
    "AGG_METADATA = \"agg_metadata.pkl.gz\"            # custom file for merged metadata\n",
    "AGG_AVFEAT = \"agg_avfeature.pkl.gz\"             # custom file for merged audio and video features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook C: Self-Service Labeling\n",
    "\n",
    "Collecting labels can be an arduous, expensive task.  That's why we're turning to an internal platform that simplifies the process with a programmatic API and democrtizes the labelers to some or all of your fellow employees.  In this notebook, we will focus on creating, exploring, and tuning a labeling campaign.  The task herein takes a momentary break away from our contextual advertising focus, but we'll use skill here to continue that direction in the next notebook.\n",
    "\n",
    "![LabelQuest](assets/labelquest_banner.jpg)\n",
    "\n",
    "[LabelQuest](https://lq.web.DOMAIN) is an AT&T labeling platform that allows task creation through programmatic API and broad label solicitation across the enterprise.  While due dilligence is still required to avoid senstive information and content, the tracking of labels and compliance-approved usage of the software on desktops, laptops, and tablets is already there.  Additionally, the gamification of the labeling task, manifesting in variance of tasks, a simple but intutitive UX, and the awarding of points and badges may keep spirits up if the number of tasks starts to build up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonic API\n",
    "Quick discussion of the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Creation\n",
    "Demo code to create a project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Creation\n",
    "Demo code to gather materials from a directory or text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Information\n",
    "Demo code to add extra information to the tasks (e.g. the info box or question additions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Type Variance\n",
    "Demo code to add text, image, or video tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation by Association\n",
    "In our user survey, the majority indicated they'd be willing to provide a few labels to a service for better product recommendations, so let's test that promise and its efficacy.  \n",
    "\n",
    "![willingness to label](assets/labelquest_agreement.jpg)\n",
    "\n",
    "In the remainder of this notebook, we'll use the [IMDB 5000 dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset) that has a small collection of movies and some metadata about actors and directors.  Unfortunately, you won't be able to make direct requests to WarnerBrothers or HBO to produce your most preferred combination, but this example should demonstrate the power of rapid model refinement with just a few labels.\n",
    "\n",
    "To jump right to the fun part, we've done a little bit of preprocessing to formulate a few models on the data.  The code in the following cell **will not be executed** because we did it for you, but it's provided here for some fun examples.\n",
    "\n",
    "1. **Actor Affinity** - finding your preferred actor by the links to others\n",
    "2. **Genre Preferences** - finding preference for genres by direct categorical links\n",
    "3. **Crowd Alignment** - how closely do your opinions match that of others, as determined by `likes`\n",
    "4. **Embedded Topics** - (advanced) a method that uses our NLP embedding to get recommendations, similar to genre work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP model and flattened featured ready to go!\n"
     ]
    }
   ],
   "source": [
    "# OLD CODE, but examples of thigns that would be run FOR the user to be used in above experiments\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from scipy import spatial\n",
    "from pathlib import Path\n",
    "\n",
    "def text2doc(nlp, tag_raw):\n",
    "    \"\"\"Given a raw text input, tokenize it to remove stop words\"\"\"\n",
    "    return [x for x in nlp(tag_raw) if not x.is_stop and not x.is_punct]\n",
    "\n",
    "def doc2vec(nlp, doc, target_domain=None):\n",
    "    \"\"\"Given a specific model, clean line of text into an output embedding space\"\"\"\n",
    "    # https://spacy.io/usage/vectors-similarity\n",
    "    if target_domain is None:\n",
    "        target_domain = nlp.vocab\n",
    "    if type(doc) != list:\n",
    "        doc = [doc]\n",
    "    tag_doc = None\n",
    "    for token in doc:\n",
    "        tag_id = target_domain.strings[token.text]\n",
    "        if tag_id in target_domain.vectors:   # search existing one\n",
    "            new_vec = target_domain.vectors[tag_id]\n",
    "        elif type(token)==str:\n",
    "            new_vec = nlp(token).vector\n",
    "        else:\n",
    "            new_vec = token.vector\n",
    "        if tag_doc is None:\n",
    "            tag_doc = new_vec\n",
    "        else:\n",
    "            tag_doc += new_vec\n",
    "    return tag_doc\n",
    "\n",
    "# doc = text2doc(nlp, \"this is a phrase to clean\")\n",
    "# vec = doc2vec(nlp, doc)\n",
    "\n",
    "# also load our spacy NLP model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "print(\"NLP model ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and embedding classes...\n",
      "Lookup specific tags by match...\n",
      "Count of new text-based mapping: 1804...\n",
      "Shape of embedded tag matrix: (1763, 300)...\n"
     ]
    }
   ],
   "source": [
    "# let's plow through our class dataframe and do the cleaning and mapping\n",
    "# print([x for x in df_classes[df_classes['primary']==1].iloc])\n",
    "print(\"Tokenizing and embedding classes...\")\n",
    "list_tokens = [\"\"] * len(df_classes)\n",
    "list_vect = [np.ndarray((1, nlp.vocab.vectors.shape[1]))] * len(df_classes)\n",
    "for idx in range(len(df_classes)):  # iterate row indexes\n",
    "    row = df_classes.iloc[idx]\n",
    "    # tokenize to remove tags, return tokens\n",
    "    doc = text2doc(nlp, row[\"definition\"]) + text2doc(nlp, row[\"class\"])\n",
    "    list_tokens[idx] = [str(x) for x in doc]\n",
    "    # convert to an embedding array\n",
    "    list_vect[idx] = doc2vec(nlp, doc)\n",
    "df_classes[\"token\"] = list_tokens\n",
    "df_classes[\"embedding\"] = list_vect\n",
    "\n",
    "# now collect all of the tags that match simple text bag\n",
    "print(\"Lookup specific tags by match...\")\n",
    "list_tags = df_flatten['tag'].unique()\n",
    "map_tokens = {}\n",
    "embed_tokens = np.ndarray((len(list_tags), nlp.vocab.vectors.shape[1]))\n",
    "for idx in range(len(list_tags)):   # iterate through full list of known tags\n",
    "    doc = text2doc(nlp, list_tags[idx])\n",
    "    for x in doc:  # create map/reference to each term\n",
    "        x = str(x)\n",
    "        if x not in map_tokens:  # first time to see this tag?\n",
    "            map_tokens[x] = []\n",
    "        map_tokens[x].append(idx)   # save reference to original tag set\n",
    "    embed_tokens[idx, :] = doc2vec(nlp, doc)  # compute embedding \n",
    "\n",
    "# df_classes[\"token\"] = list_tokens\n",
    "# df_classes[\"embedding\"] = list_vect\n",
    "print(f\"Count of new text-based mapping: {len(map_tokens)}...\")\n",
    "print(f\"Shape of embedded tag matrix: {embed_tokens.shape}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few Labels\n",
    "What does it look like when someone added labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking of Tasks\n",
    "How do you rerank or prioritize the tasks with label data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Labeling Material\n",
    "\n",
    "Ready to label the world?  Just remember that out-of-context, your friends and family may not appreciate you giving them a `happy` or `sad` label, and much less so when you try to click a submit button afterwards! With the familiarity to a labeling system underway, we have the skills to return to task for content labeling.\n",
    "\n",
    "The next notebook, [notebook D](D_active_labels.ipynb) *(that link may not work)* returns to the contextual ads problem and applies what was learned here to the labeling task that everyone contributed to (hopefully!) before the workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
