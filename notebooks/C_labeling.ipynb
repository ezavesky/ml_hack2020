{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning Workshop: Content Insights 2020](assets/mlci_banner.jpg)\n",
    "\n",
    "# Machine Learning Workshop: Content Insights 2020\n",
    "\n",
    "Welcome to the workshop notebooks!  These notebooks are designed to give you a walk through the steps of creating a model, refining it with user labels, and testing it on content.  You can access the main [workshop forum page](https://INFO_SITE/forums/html/forum?id=241a0b77-7aa6-4fef-9f25-5ea351825725&ps=25), the [workshop files repo](https://INFO_SITE/communities/service/html/communityview?communityUuid=fb400868-b17c-44d8-8b63-b445d26a0be4#fullpageWidgetId=W403a0d6f86de_45aa_8b67_c52cf90fca16&folder=d8138bef-9182-4bdc-8b12-3c88158a219c), or the [symposium home page](https://software.web.DOMAIN) for additional help.\n",
    "\n",
    "The notebooks are divided into five core components: (A) setup & data, (B) model exploration, (C) labeling, (D) active labeling, (E) and deployment.  You are currently viewing the *setup & data* workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for running the workshop; we'll repeat these in the top line of each workbook.\n",
    "#   why repeat them? the backup routine only serializes .ipynb files, so others will need \n",
    "#   to be downloaded again if your compute instance restarts (a small price to pay, right?)\n",
    "\n",
    "WORKSHOP_BASE = \"https://vmlr-workshop.STORAGE\"\n",
    "# WORKSHOP_BASE = \"http://content.research.DOMAIN/projects/mlci_2020\"\n",
    "AGG_METADATA = \"agg_metadata.pkl.gz\"            # custom file for merged metadata\n",
    "\n",
    "LQ_JWT = \"\"  # you need to provide this (copy the string from https://APP_SITE/api/lq/v1/uam/auth)\n",
    "LQ_ROOT_URL = \"https://APP_SITE\"\n",
    "LQ_ROOT_SSL_VERIFY = False\n",
    "\n",
    "IMDB5000_FEAT = \"packages/movie_metadata.csv\"   # public dataset for movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook C: Self-Service Labeling\n",
    "\n",
    "Collecting labels can be an arduous, expensive task.  That's why we're turning to an internal platform that simplifies the process with a programmatic API and democrtizes the labelers to some or all of your fellow employees.  In this notebook, we will focus on creating, exploring, and tuning a labeling campaign.  The task herein takes a momentary break away from our contextual advertising focus, but we'll use skill here to continue that direction in the next notebook.\n",
    "\n",
    "![LabelQuest](assets/labelquest_banner.jpg)\n",
    "\n",
    "[LabelQuest](https://lq.web.DOMAIN) is an AT&T labeling platform that allows task creation through programmatic API and broad label solicitation across the enterprise.  While due dilligence is still required to avoid senstive information and content, the tracking of labels and compliance-approved usage of the software on desktops, laptops, and tablets is already there.  Additionally, the gamification of the labeling task, manifesting in variance of tasks, a simple but intutitive UX, and the awarding of points and badges may keep spirits up if the number of tasks starts to build up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonic API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lq\n",
    "from lq.content_label import ContentLabeler\n",
    "\n",
    "if not LQ_JWT:\n",
    "    LQ_JWT = ContentLabeler.jwt_load(\"auth.json\")\n",
    "if not LQ_JWT:\n",
    "    raise Exception(\"\"\"\n",
    "        No token detected (in LQ_JWT), please authenticate and get your JWT token.\n",
    "        1. Log into the test instance of LQ - https://APP_SITE/\n",
    "        2a. Get your LQ token from here - https://APP_SITE/api/lq/v1/uam/auth\n",
    "        2b. OR Save the produced JSON file to the same directory as this script (as auth.json)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Creation\n",
    "This cell demonstrates basic functions for listing, retrieving and creating new projects.  To avoid any confusion, the code discussions will use the term `project` instead of `campaign` but otherwise they are meant to be interchangable in this workshop.\n",
    "\n",
    "The sample below demos these functions...\n",
    "* `ContentLabeler.list()` - queries active projects with \n",
    "* `ContentLabeler.tasks_retrieve()` - query the tasks under a single project with \n",
    "* `ContentLabeler.load()` - create or load an existing project\n",
    "* `ContentLabeler.delete()` - delete a project that has been loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8893a793e2dd409894d76f7e8bc6e5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Project:', layout=Layout(width='90%'), options=(\"created_topic: Project 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8472b3935a740858ca49a02a82dd6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "\n",
    "# first, create a handy content labeling instance\n",
    "labeler = ContentLabeler(LQ_JWT, LQ_ROOT_URL, verbose=False, ssl_verify=LQ_ROOT_SSL_VERIFY)\n",
    "\n",
    "def proj_create_ex(proj_title=\"emotion_check\"):\n",
    "    # let's create a simple textual campaign\n",
    "    labeler.load(proj_title, create_if_missing=True)\n",
    "    if not labeler.valid:\n",
    "        raise Exception(\"\"\"\n",
    "        Uh oh, failed to create a new project.  Make sure your token is valid and check above errors.\"\"\")\n",
    "    list_classes = [\"happy\", \"sad\", \"neutral\"]\n",
    "    list_media = [\"A juicy apple\", \"A long-time friend\", \"In-laws\", \"Tax day\"]\n",
    "    text_questions = \"Please select your primary emotion when thinking of these items\"\n",
    "    num_inserted = labeler.tasks_insert(list_classes, list_media, is_exclusive=True, data_question=text_questions)\n",
    "    if num_inserted == 0:  # hrm, it says no new inserts, let's check for a prior project\n",
    "        list_tasks = labeler.tasks_retrieve(0)   # -1=label only, 0=all, 1=unlabeled\n",
    "        print(f\"The project '{proj_title}' already exists, with {len(list_tasks)} tasks of expected {len(list_media)} tasks\")\n",
    "        assert len(list_tasks) == len(list_media)\n",
    "    else:\n",
    "        print(f\"The project '{proj_title}' was created, with {num_inserted} tasks of expected {len(list_media)} tasks\")\n",
    "        assert num_inserted == len(list_media)\n",
    "    print(f\"That's it for basic project creation, head to LQ to check it out: {LQ_ROOT_URL}\")\n",
    "\n",
    "\n",
    "# create a quick interaction grid for display\n",
    "def proj_view(create_fn, template_name, proj=None, delete=False, create=False):\n",
    "    global df_proj\n",
    "    def refresh():\n",
    "        df_proj = pd.DataFrame(labeler.list())\n",
    "        df_proj['id'] = df_proj['title'].map(str) + \": \" + df_proj['description'].map(str) \n",
    "        df_proj.set_index('id', inplace=True)\n",
    "        return df_proj\n",
    "    \n",
    "    if proj is None:\n",
    "        df_proj = refresh()\n",
    "        dropdown.options = list(df_proj.index)\n",
    "        return\n",
    "    if not proj in df_proj.index:\n",
    "        print(f\"Error: Couldn't find the specified project {proj} in results!\")\n",
    "        return\n",
    "    if delete:\n",
    "        result = labeler.delete()\n",
    "        df_proj = refresh()\n",
    "        btn_del.value = False\n",
    "        dropdown.options = list(df_proj.index)\n",
    "        dropdown.value = df_proj.index[0]\n",
    "        return\n",
    "    if create:\n",
    "        title_new = f\"{template_name}_{len(df_proj)}\"\n",
    "        result = create_fn(title_new)\n",
    "        df_proj = refresh()\n",
    "        idx_new = df_proj[df_proj[\"title\"]==title_new].index[0]\n",
    "        btn_create.value = False\n",
    "        dropdown.options = list(df_proj.index)\n",
    "        dropdown.value = idx_new\n",
    "        return\n",
    "\n",
    "    # display(f\"Matched Tags (for class {class_name}): {df_performance.loc[run_name, 'token']}\")\n",
    "    proj_name = df_proj.loc[proj, \"title\"]\n",
    "    labeler.load(proj_name, create_if_missing=False)\n",
    "    \n",
    "    if labeler.valid:\n",
    "        df_tasks = pd.DataFrame(labeler.tasks_retrieve(0))\n",
    "        df_tasks_sub = df_tasks[['etag', 'data', 'labels']]\n",
    "        display(df_tasks_sub)\n",
    "    else:\n",
    "        print(f\"No tasks found for project {proj_name}\")\n",
    "\n",
    "df_proj = None  # start with empty obj\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=[] if df_proj is None else list(df_proj.index),  # send run names\n",
    "    description='Project:', layout={'width': '90%'},\n",
    "    disabled=False,\n",
    ")\n",
    "btn_del = widgets.ToggleButton(description=\"Delete\")\n",
    "btn_create = widgets.ToggleButton(description=\"Create\")\n",
    "\n",
    "# create two rows ; \"partial\" allows us to sub in another function for create\n",
    "demo_fn = partial(proj_view, proj_create_ex, \"new_project\")\n",
    "out = widgets.interactive_output(demo_fn, {'proj':dropdown, 'delete':btn_del, 'create':btn_create})\n",
    "\n",
    "display(widgets.VBox([dropdown, widgets.HBox([btn_del, btn_create])]), out)\n",
    "demo_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Creation\n",
    "This cell demonstrates how to create a project out of assets that are found in a directory.  To demonstrate, the code will alternate between text-based tasks and image-based ones.\n",
    "\n",
    "The following methods are demonstrated here.\n",
    "* (media collection + determination)\n",
    "* (extra information about the media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36b6112d2314da9981d5404b58e6ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Project:', layout=Layout(width='90%'), options=(\"created_topic: Project 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e13ded057cb41c6a974c474999438e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def proj_task_find_ex(proj_title=\"emotion_check\"):\n",
    "    # let's create a simple textual campaign\n",
    "    print(\"IN TASK FIND CREATE\")\n",
    "    labeler.load(proj_title, create_if_missing=True)\n",
    "    if not labeler.valid:\n",
    "        raise Exception(\"\"\"\n",
    "        Uh oh, failed to create a new project.  Make sure your token is valid and check above errors.\"\"\")\n",
    "    list_classes = [\"happy\", \"sad\", \"neutral\"]\n",
    "    list_media = [\"New task, maybe with HTML\"]\n",
    "    text_questions = \"Please select your primary emotion when thinking of these items\"\n",
    "    num_inserted = labeler.tasks_insert(list_classes, list_media, is_exclusive=True, data_question=text_questions)\n",
    "    if num_inserted == 0:  # hrm, it says no new inserts, let's check for a prior project\n",
    "        list_tasks = labeler.tasks_retrieve(0)   # -1=label only, 0=all, 1=unlabeled\n",
    "        print(f\"The project '{proj_title}' already exists, with {len(list_tasks)} tasks of expected {len(list_media)} tasks\")\n",
    "        assert len(list_tasks) == len(list_media)\n",
    "    else:\n",
    "        print(f\"The project '{proj_title}' was created, with {num_inserted} tasks of expected {len(list_media)} tasks\")\n",
    "        assert num_inserted == len(list_media)\n",
    "    print(f\"That's it for basic project creation, head to LQ to check it out: {LQ_ROOT_URL}\")\n",
    "\n",
    "\n",
    "# create two rows ; \"partial\" allows us to sub in another function for create\n",
    "\n",
    "demo2_fn = partial(proj_view, proj_task_find_ex, \"discovered_tasks\")\n",
    "out = widgets.interactive_output(demo2_fn, {'proj':dropdown, 'delete':btn_del, 'create':btn_create})\n",
    "\n",
    "display(widgets.VBox([dropdown, widgets.HBox([btn_del, btn_create])]), out)\n",
    "demo2_fn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Type Variance\n",
    "Demo code to add text, image, or video tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation by Association\n",
    "In our user survey, the majority indicated they'd be willing to provide a few labels to a service for better product recommendations, so let's test that promise and its efficacy.  \n",
    "\n",
    "![willingness to label](assets/labelquest_agreement.jpg)\n",
    "\n",
    "In the remainder of this notebook, we'll use the [IMDB 5000 dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset) that has a small collection of movies and some metadata about actors and directors.  Unfortunately, you won't be able to make direct requests to WarnerBrothers or HBO to produce your most preferred combination, but this example should demonstrate the power of rapid model refinement with just a few labels.\n",
    "\n",
    "To jump right to the fun part, we've done a little bit of preprocessing to formulate a few models on the data.  The code in the following cell **will not be executed** because we did it for you, but it's provided here for some fun examples.\n",
    "\n",
    "1. **Actor Affinity** - finding your preferred actor by the links to others\n",
    "2. **Genre Preferences** - finding preference for genres by direct categorical links\n",
    "3. **Crowd Alignment** - how closely do your opinions match that of others, as determined by `likes` or `box office gross`\n",
    "4. **Embedded Topics** - (advanced) a method that uses our NLP embedding to get recommendations, similar to genre work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB5000_FEAT - train some simple models according to above?\n",
    "df_movies = pd.read_csv(IMDB5000_FEAT)\n",
    "print(df_movies.columns, len(df_movies))\n",
    "\n",
    "# task 1: encode all features into movie-based rows\n",
    "# task 2: select actors by affinity to likes \n",
    "\n",
    "\n",
    "\n",
    "df_movies['people'] = df_movies['director_name'].map(str) + \"|\" + df_movies['actor_1_name'].map(str) + \\\n",
    "                         \"|\" + df_movies['actor_2_name'].map(str) + \"|\" + df_movies['actor_3_name'].map(str)\n",
    "df_movies['people'] = df_movies['people'].apply(lambda x: x.split('|'))\n",
    "df_movies['genres'] = df_movies['genres'].apply(lambda x: x.split('|'))\n",
    "df_movies = df_movies.explode('people').explode('genres')\n",
    "\n",
    "print(df_movies.columns, len(df_movies))\n",
    "df_movies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# OLD CODE, but examples of thigns that would be run FOR the user to be used in above experiments\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from scipy import spatial\n",
    "from pathlib import Path\n",
    "\n",
    "def text2doc(nlp, tag_raw):\n",
    "    \"\"\"Given a raw text input, tokenize it to remove stop words\"\"\"\n",
    "    return [x for x in nlp(tag_raw) if not x.is_stop and not x.is_punct]\n",
    "\n",
    "def doc2vec(nlp, doc, target_domain=None):\n",
    "    \"\"\"Given a specific model, clean line of text into an output embedding space\"\"\"\n",
    "    # https://spacy.io/usage/vectors-similarity\n",
    "    if target_domain is None:\n",
    "        target_domain = nlp.vocab\n",
    "    if type(doc) != list:\n",
    "        doc = [doc]\n",
    "    tag_doc = None\n",
    "    for token in doc:\n",
    "        tag_id = target_domain.strings[token.text]\n",
    "        if tag_id in target_domain.vectors:   # search existing one\n",
    "            new_vec = target_domain.vectors[tag_id]\n",
    "        elif type(token)==str:\n",
    "            new_vec = nlp(token).vector\n",
    "        else:\n",
    "            new_vec = token.vector\n",
    "        if tag_doc is None:\n",
    "            tag_doc = new_vec\n",
    "        else:\n",
    "            tag_doc += new_vec\n",
    "    return tag_doc\n",
    "\n",
    "# doc = text2doc(nlp, \"this is a phrase to clean\")\n",
    "# vec = doc2vec(nlp, doc)\n",
    "\n",
    "# also load our spacy NLP model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "print(\"NLP model ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plow through our class dataframe and do the cleaning and mapping\n",
    "# print([x for x in df_classes[df_classes['primary']==1].iloc])\n",
    "print(\"Tokenizing and embedding classes...\")\n",
    "list_tokens = [\"\"] * len(df_classes)\n",
    "list_vect = [np.ndarray((1, nlp.vocab.vectors.shape[1]))] * len(df_classes)\n",
    "for idx in range(len(df_classes)):  # iterate row indexes\n",
    "    row = df_classes.iloc[idx]\n",
    "    # tokenize to remove tags, return tokens\n",
    "    doc = text2doc(nlp, row[\"definition\"]) + text2doc(nlp, row[\"class\"])\n",
    "    list_tokens[idx] = [str(x) for x in doc]\n",
    "    # convert to an embedding array\n",
    "    list_vect[idx] = doc2vec(nlp, doc)\n",
    "df_classes[\"token\"] = list_tokens\n",
    "df_classes[\"embedding\"] = list_vect\n",
    "\n",
    "# now collect all of the tags that match simple text bag\n",
    "print(\"Lookup specific tags by match...\")\n",
    "list_tags = df_flatten['tag'].unique()\n",
    "map_tokens = {}\n",
    "embed_tokens = np.ndarray((len(list_tags), nlp.vocab.vectors.shape[1]))\n",
    "for idx in range(len(list_tags)):   # iterate through full list of known tags\n",
    "    doc = text2doc(nlp, list_tags[idx])\n",
    "    for x in doc:  # create map/reference to each term\n",
    "        x = str(x)\n",
    "        if x not in map_tokens:  # first time to see this tag?\n",
    "            map_tokens[x] = []\n",
    "        map_tokens[x].append(idx)   # save reference to original tag set\n",
    "    embed_tokens[idx, :] = doc2vec(nlp, doc)  # compute embedding \n",
    "\n",
    "# df_classes[\"token\"] = list_tokens\n",
    "# df_classes[\"embedding\"] = list_vect\n",
    "print(f\"Count of new text-based mapping: {len(map_tokens)}...\")\n",
    "print(f\"Shape of embedded tag matrix: {embed_tokens.shape}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few Labels\n",
    "What does it look like when someone added labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking of Tasks\n",
    "How do you rerank or prioritize the tasks with label data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Labeling Material\n",
    "\n",
    "Ready to label the world?  Just remember that out-of-context, your friends and family may not appreciate you giving them a `happy` or `sad` label, and much less so when you try to click a submit button afterwards! With the familiarity to a labeling system underway, we have the skills to return to task for content labeling.\n",
    "\n",
    "The next notebook, [notebook D](D_active_labels.ipynb) *(that link may not work)* returns to the contextual ads problem and applies what was learned here to the labeling task that everyone contributed to (hopefully!) before the workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
