{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning Workshop: Content Insights 2020](assets/mlci_banner.jpg)\n",
    "\n",
    "# Machine Learning Workshop: Content Insights 2020\n",
    "\n",
    "Welcome to the workshop notebooks!  These notebooks are designed to give you a walk through the steps of creating a model, refining it with user labels, and testing it on content.  You can access the main [workshop forum page](https://INFO_SITE/forums/html/forum?id=241a0b77-7aa6-4fef-9f25-5ea351825725&ps=25), the [workshop files repo](https://INFO_SITE/communities/service/html/communityview?communityUuid=fb400868-b17c-44d8-8b63-b445d26a0be4#fullpageWidgetId=W403a0d6f86de_45aa_8b67_c52cf90fca16&folder=d8138bef-9182-4bdc-8b12-3c88158a219c), or the [symposium home page](https://software.web.DOMAIN) for additional help.\n",
    "\n",
    "The notebooks are divided into five core components: (A) setup & data, (B) model exploration, (C) labeling, (D) active labeling, (E) and deployment.  You are currently viewing the *setup & data* workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for running the workshop; we'll repeat these in the top line of each workbook.\n",
    "#   why repeat them? the backup routine only serializes .ipynb files, so others will need \n",
    "#   to be downloaded again if your compute instance restarts (a small price to pay, right?)\n",
    "\n",
    "WORKSHOP_BASE = \"https://vmlr-workshop.STORAGE\"\n",
    "# WORKSHOP_BASE = \"http://content.research.DOMAIN/projects/mlci_2020\"\n",
    "AGG_PROCESSED = \"models/agg_processed.pkl.gz\"     # custom file for processed metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook E: Deployment\n",
    "\n",
    "We're repeating the exploration of compressed timed metadata by first merging it into some more easily usable [pandas DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html).  Specifically, we'll merge the raw output from many assets and content analysis tools.  This should look familiar!\n",
    "\n",
    "In this rest of this notebook, we'll take quick look at what a deployed extractor may generate.  If you want to know more about deployment itself, head over to [Contentai Extractors](https://www.contentai.io/docs/extractor-getting-started) and get started.  Feel free to skip to the [appendix](#Appendix---Extractor-Sample) for example code in your own extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping re-create of metadata file 'models/agg_processed.pkl.gz'...\n",
      "New columns in this data... ['time_begin', 'source_event', 'tag_type', 'time_end', 'time_event', 'tag', 'score', 'details', 'extractor', 'asset']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "path_metadata = Path(AGG_PROCESSED)\n",
    "if path_metadata.exists():\n",
    "    print(f\"Skipping re-create of metadata file '{str(path_metadata)}'...\")\n",
    "    df_flatten = pd.read_pickle(str(path_metadata))\n",
    "else:\n",
    "    df_flatten = None\n",
    "    num_files = 0\n",
    "    path_content = Path(\"packages/trailers\")\n",
    "    list_files = list(path_content.rglob(\"csv_flatten*.csv*\"))\n",
    "    print(f\"Ingesting {len(list_files)} flatten files in path '{str(path_content)}'...\")\n",
    "    for path_file in list_files:  # search for flattened files\n",
    "        df_new = pd.read_csv(path_file)\n",
    "        # FROM content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz -> \n",
    "        # TO halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten (relative_to)\n",
    "        # TO halloween/vid_halloween_0-13-of-23.mp4  (joining base path parts)\n",
    "        path_asset = Path(*path_file.parent.relative_to(path_content).parts[:2])\n",
    "        df_new['tag'] = df_new['tag'].str.lower()   # lower case the tags\n",
    "        df_new['details'] = df_new['details'].fillna('').str.lower()   # lower case the enhanced information\n",
    "        df_new['asset'] = str(path_asset)\n",
    "        if df_flatten is None:   # first one we saw\n",
    "            df_flatten = df_new\n",
    "        else:\n",
    "            df_flatten = df_flatten.append(df_new, ignore_index=True)   # append new dataframe\n",
    "        num_files += 1\n",
    "        if num_files % 500 == 0:\n",
    "            print(f\"... read {num_files}...\")\n",
    "            \n",
    "    # experimental code to parse halloween output\n",
    "    path_content = Path(\"packages/trailers\")\n",
    "    list_files = list(path_content.rglob(\"*halloween*/*.json*\"))\n",
    "    print(list_files)\n",
    "    obj_halloween = None\n",
    "    for path_local in list_files:\n",
    "        with list_files[0].open('r') as f:\n",
    "            obj_halloween = json.load(f)\n",
    "        df_new = pd.DataFrame(obj_halloween['results'])\n",
    "        # print(json.dumps(obj_halloween, indent=4))\n",
    "        df_new[\"time_event\"] = df_new[\"time_begin\"]\n",
    "        df_new[\"tag_type\"] = \"tag\"\n",
    "        df_new[\"tag\"] = \"halloween\"\n",
    "        df_new[\"source_event\"] = \"video\"\n",
    "        df_new[\"extractor\"] = \"dsai_activity_halloween\"\n",
    "        df_new.drop(columns=[\"type_audio\", \"type_video\", \"path_video\", \"class\", \"id\"], inplace=True)\n",
    "        df_new[\"details\"] = \"\"\n",
    "        path_local = Path(*path_local.parent.relative_to(path_content).parts[:2])\n",
    "        df_new[\"asset\"] = str(path_local)\n",
    "        \n",
    "        if df_flatten is None:\n",
    "            df_flatten = df_new\n",
    "        else:\n",
    "            df_flatten = df_flatten.append(df_new, ignore_index=True)   # append new dataframe\n",
    "            num_files += 1            \n",
    "    df_flatten.reset_index(drop=True, inplace=True)  # drop prior index\n",
    "    df_flatten.to_pickle(str(path_metadata))\n",
    "    print(f\"Wrote {num_files} aggregations to file '{str(path_metadata)}'...\")\n",
    "    \n",
    "print(f\"New columns in this data... {list(df_flatten.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting tag statistics\n",
    "Let's plot some statistics about tags, both their numbers and their names.  First, a histogram of how many unique and total tags were present for an asset.  This plot helps us find average number of tags, both in raw counts and unique tags for an asset.  Second, an average and raw count of the top `N` tags found from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e08e8d9d91c4869a3d6c0960062167c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatRangeSlider(value=(0.5, 1.0), continuous_update=False, description='Score Range:', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ac3a66ed4246fdb748aac2df4002ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b33bf5f098c4a908f03cfa23540c794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatRangeSlider(value=(0.5, 1.0), continuous_update=False, description='Score Range:', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import YouTubeVideo\n",
    "# from IPython import display\n",
    "\n",
    "VIDEO_URL=\"go6GEIrcvFY\"\n",
    "\n",
    "\n",
    "# this is a handy update function\n",
    "def tag_video(x=None, clip=None, tag=None):\n",
    "    \n",
    "    # print(clip)\n",
    "    if tag is None:\n",
    "        tag = \"halloween\"\n",
    "    \n",
    "    df_sub = df_flatten[(df_flatten['score'] >= x[0]) & (df_flatten['score'] <= x[1])]\n",
    "    df_history = df_sub[df_sub[\"tag\"]==tag]\n",
    "    df_rev = df_history.sort_values([\"score\"], ascending=False)\n",
    "    df_rev[\"jump\"] = f\"{tag}, \"+df_rev[\"score\"].round(2).map(str) + \"@ , \" + \\\n",
    "        df_rev[\"time_begin\"].round(2).map(str) + \", \" + df_rev[\"time_end\"].round(2).map(str)\n",
    "\n",
    "    if clip is not None and len(df_rev):\n",
    "        time_parts = clip.split(',')\n",
    "        if tag != time_parts[0].strip():\n",
    "            clip_dropdown.options = list(df_rev[\"jump\"].head(10))\n",
    "            return\n",
    "\n",
    "    time_start = 0\n",
    "    time_end = None\n",
    "    if len(df_rev):\n",
    "        # print(df_rev.head(10))\n",
    "        time_start = 0 if not len(df_rev) else int(df_rev.head(1)[\"time_begin\"])\n",
    "        time_end = time_start+5 if not len(df_rev) else int(df_rev.head(1)[\"time_end\"])\n",
    "        if not clip_dropdown.options: \n",
    "            clip_dropdown.options = list(df_rev[\"jump\"].head(10))\n",
    "            return\n",
    "    if clip is not None:\n",
    "        time_parts = clip.split(',')\n",
    "        time_start = int(float(time_parts[-2].strip()))\n",
    "        time_end = int(float(time_parts[-1].strip()))\n",
    "    vid = YouTubeVideo(VIDEO_URL, start=time_start, end=time_end, loop=1)\n",
    "    print(f\"Video Start: {time_start}, End: {time_end}\")\n",
    "    with out_v:\n",
    "        out_v.clear_output()\n",
    "        display(vid)\n",
    "\n",
    "\n",
    "# this is a handy update function\n",
    "def tag_count_hist(x=None):\n",
    "    \n",
    "    x = (round(x[0], 2), round(x[1], 2))\n",
    "    df_sub = df_flatten[(df_flatten['score'] >= x[0]) & (df_flatten['score'] <= x[1])]\n",
    "    df_history = df_sub[df_sub[\"tag\"]==\"halloween\"]\n",
    "    f, ax = plt.subplots(figsize=(12,2))\n",
    "    ax.stem(df_history[\"time_event\"], df_history['score'], use_line_collection=True)\n",
    "    ax.set_title(f\"Instances of Halloween ({x[0]} >= Score >= {x[1]})\")\n",
    "    ax.set_ylabel('score')\n",
    "    ax.set_xlabel('time (in seconds)')\n",
    "    ax.set_xlim([0.0, df_flatten['time_end'].max()])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.grid()\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "    df_pairs = df_sub.groupby(['tag','asset']).count()['score'].reset_index()   # group by two params, reset into dataframe\n",
    "    df_unitags = df_pairs.groupby(['tag'])['score'].agg(['count','sum']).reset_index()   # group by asset to find unique tag count per asset\n",
    "    df_unitags.sort_values('sum', ignore_index=True, inplace=True, ascending=False)\n",
    "    df_unitags.rename(columns={\"count\":\"Asset Frequency\", \"sum\":\"Total Frequency\"}, inplace=True)\n",
    "    top_n = 20\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "    \n",
    "    df_topn = df_unitags.iloc[:top_n]\n",
    "    df_topn.plot.barh(ax=ax1, x='tag', width=0.8, log=True)\n",
    "    ax1.set_title(f\"Top {top_n} Tags ({x[0]} >= Score >= {x[1]})\")\n",
    "    ax1.set_ylabel('tag text')\n",
    "    ax1.set_xlabel('count of tags')\n",
    "    ax1.legend(loc=\"lower left\")\n",
    "    ax1.grid()\n",
    "    \n",
    "    skip_percent = 0.05\n",
    "    top_percent = int(len(df_unitags)*skip_percent)\n",
    "    df_topn = df_unitags.iloc[top_percent:top_percent+top_n]\n",
    "    df_topn.plot.barh(ax=ax2, x='tag', width=0.8, log=True)\n",
    "    ax2.set_title(f\"Top {top_n} (skip {skip_percent*100:1}%) Tags ({x[0]} >= Score >= {x[1]})\")\n",
    "    ax2.set_ylabel('')\n",
    "    ax2.set_xlabel('count of tags')\n",
    "    ax2.legend(loc=\"lower left\")\n",
    "    ax2.grid()\n",
    "    \n",
    "    tag_dropdown.options = [\"halloween\"] + list(df_topn[\"tag\"])\n",
    "    clip_dropdown.options = []\n",
    "    \n",
    "    #plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "# get an interactive widget/graph\n",
    "range_slider = widgets.FloatRangeSlider(\n",
    "    value=[0.5, 1.0],\n",
    "    step=0.05,\n",
    "    min=df_flatten['score'].min(),\n",
    "    max=df_flatten['score'].max(),\n",
    "    description='Score Range:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    ")\n",
    "tag_dropdown = widgets.Dropdown(\n",
    "    options=list(),  # send run names\n",
    "    description='Tag:',\n",
    "    disabled=False,\n",
    ")\n",
    "clip_dropdown = widgets.Dropdown(\n",
    "    options=list(),  # send run names\n",
    "    description='Clip:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "out = widgets.interactive(tag_count_hist, x=range_slider)\n",
    "out_v = widgets.Output()\n",
    "out2 = widgets.interactive(tag_video, tag=tag_dropdown, clip=clip_dropdown, x=range_slider)\n",
    "# out.layout.height = '575px'  # disable this if you make your output window longer!\n",
    "display(out2, out_v, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - Extractor Sample\n",
    "\n",
    "This is a functional main file that could get called within a ContentAI extractor.  It uses OpenCV to read a video or a frame and perform some model analysis (**NOTE** no reall model evaluation here, it's just an example!).  Other trappings to turn this into a full fledged extractor include a manifest file  with extractor information and a simple Docker file; [see the documentation](https://www.contentai.io/docs/extractor-getting-started) for more exact formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disabled in notebook mode...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "import cv2\n",
    "\n",
    "import contentaiextractor as contentai\n",
    "\n",
    "\n",
    "def classify_image(model, nd_image=None, path_image=None):\n",
    "\n",
    "    # # Predict single image\n",
    "    # predict.classify(model, '2.jpg')\n",
    "    # # {'2.jpg': {'magic': 4.3454722e-05, 'natural': 0.85139805}}\n",
    "\n",
    "    # # Predict multiple images at once\n",
    "    # predict.classify(model, ['/Users/bedapudi/Desktop/2.jpg', '/Users/bedapudi/Desktop/6.jpg'])\n",
    "    # # {'2.jpg': {'magic': 0.14751942, 'natural': 0.8513979}, '6.jpg': {'magic': 0.013342537, 'natural': 0.5209196}}\n",
    "\n",
    "    \n",
    "    if path_image is None:   # plan for future where we can use nd_image prediction\n",
    "        return None\n",
    "    return {\"img\": {\"magic\":0.5, \"natural\":0.5}}\n",
    "\n",
    "\n",
    "def video_stats(cap):\n",
    "    \"\"\"Gets video stats. OpenCV is not reliable supply us with total frame count or milliseconds,\n",
    "    so we have to manually calculate them by iterating over the frames ourselves.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cap : VideoCapture\n",
    "        object for video capturing from video files\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    total_milliseconds\n",
    "        total video duration in milliseconds\n",
    "    total_frames\n",
    "        total frame count from video duration\n",
    "    \"\"\"\n",
    "\n",
    "    total_milliseconds = 0\n",
    "    total_frames = 0\n",
    "   \n",
    "    # jump to the end of the video and get the current milliseconds and frame number\n",
    "    cap.set(cv2.CAP_PROP_POS_AVI_RATIO, 1)\n",
    "    total_milliseconds = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "\n",
    "    # reset to frame zero\n",
    "    cap.set(cv2.CAP_PROP_POS_AVI_RATIO, 0)\n",
    "\n",
    "    print(f\"[INFO] total milliseconds: {total_milliseconds}\")\n",
    "    print(f\"[INFO] total frames: {total_frames}\")\n",
    "\n",
    "    return total_milliseconds, total_frames\n",
    "\n",
    "\n",
    "def analyze_dir(model, content_path, verbose=False, match_pattern=\"*.jpg\", round_decimals=5):\n",
    "    \"\"\"Run for all results in a directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content_path : str\n",
    "        The local path of the directory we want to analyze\n",
    "    verbose: bool\n",
    "        verbose printing of processing at regular intervals\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        an array of objects representing keypoints identified in the frames\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    idx_frame = 0\n",
    "    path_source = Path(content_path).resolve()\n",
    "    for path_image in sorted(path_source.rglob(match_pattern)):\n",
    "        # run inference on image from video\n",
    "        start_inference = cv2.getTickCount() / float(1000)\n",
    "        path_full = str(path_image)\n",
    "        response = classify_image(model, path_image=path_full)\n",
    "\n",
    "        # total inference time for the frame\n",
    "        inference_time = ((cv2.getTickCount() / float(1000)) - start_inference) / cv2.getTickFrequency()\n",
    "        if response is not None and path_full in response:\n",
    "            result = response[path_full]\n",
    "            for key_name in result:  # round decimals\n",
    "                result[key_name] = round(result[key_name], round_decimals)\n",
    "            result['time_event'] = 0\n",
    "            result['name_event'] = str(path_image.relative_to(path_source))\n",
    "            result['index_frame'] = idx_frame\n",
    "            results.append(result)\n",
    "        else:\n",
    "            print(f\"Warning: Classifications not found at index {idx_frame} using input file {path_image.name}\")\n",
    "        idx_frame += 1\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_video(model, content_path, time_interval=None, verbose=False, round_decimals=5):\n",
    "    \"\"\"Gets all results from running inference on images in the entire video file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content_path : str\n",
    "        The local path the the video file we want to analyze\n",
    "    time_interval : float, optional\n",
    "        how often do we want to sample (e.g. time between sample)\n",
    "    verbose: bool\n",
    "        verbose printing of processing at regular intervals\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        an array of objects representing keypoints identified in the video frames\n",
    "    \"\"\"\n",
    "    \n",
    "    # use OpenCV to open video file\n",
    "    cap = cv2.VideoCapture(content_path)\n",
    "    print(f\"[INFO] asset: {content_path}\")\n",
    "\n",
    "    # get total milliseconds\n",
    "    total_milliseconds, total_frames = video_stats(cap)\n",
    "    \n",
    "    PRINT_INTERVAL = 60 * 1000\n",
    "    time_print_next = 0   # next print time\n",
    "\n",
    "    # images_per_second = 5 #\n",
    "    fps = round(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # get temp item\n",
    "    obj_temp = tempfile.NamedTemporaryFile(suffix=\".jpg\", delete=False)\n",
    "    obj_temp.close()\n",
    "\n",
    "    hop = None\n",
    "    if time_interval != None:\n",
    "        hop = round(fps * time_interval)\n",
    "    print(f\"Info: Detected HOP interval {hop} from fps {fps} and time_interval {time_interval}\")\n",
    "\n",
    "    # holds all results from inference call\n",
    "    results = []\n",
    "    while (cap.isOpened()):\n",
    "\n",
    "        # get frame from the video\n",
    "        hasFrame, frame = cap.read()\n",
    "\n",
    "        # Stop the program if we've reached the end of the video\n",
    "        if not hasFrame:\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Release device\n",
    "            cap.release()\n",
    "            break\n",
    "\n",
    "        # get current milliseconds and frame number\n",
    "        milliseconds = cap.get(0)\n",
    "        frame_number = int(cap.get(1))\n",
    "\n",
    "        if hop is None or (frame_number % hop == 0):\n",
    "            # cv2.imwrite(obj_temp.name, frame)\n",
    "            start_inference = cv2.getTickCount()\n",
    "\n",
    "            # run inference on image from video\n",
    "            response = classify_image(model, path_image=obj_temp.name, nd_image=frame)\n",
    "            complete_inference = cv2.getTickCount()\n",
    "            time_in_sec = milliseconds / float(1000)\n",
    "\n",
    "            # total inference time for the frame\n",
    "            inference_time = (complete_inference - start_inference) / cv2.getTickFrequency()\n",
    "            if milliseconds > time_print_next:\n",
    "                time_print_next += PRINT_INTERVAL\n",
    "                if verbose:\n",
    "                    print(f\"[INFO] video {time_in_sec}s | frame: {frame_number}/{total_frames} | inference: {str(round(inference_time, 2))}s\")\n",
    "\n",
    "            if response is not None and obj_temp.name in response:\n",
    "                result = response[obj_temp.name]\n",
    "                for key_name in result:  # round decimals\n",
    "                    result[key_name] = round(result[key_name], round_decimals)\n",
    "                result['time_event'] = time_in_sec\n",
    "                result['index_frame'] = frame_number\n",
    "                results.append(result)\n",
    "            else:\n",
    "                print(f\"Warning: Classifications not found at time {time_in_sec}s using temp file {obj_temp.name}\")\n",
    "\n",
    "    # delete temp file used for writing frames\n",
    "    if obj_temp is not None:\n",
    "        path_temp = Path(obj_temp.name)\n",
    "        if path_temp.exists():\n",
    "            path_temp.unlink()\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "def classify(input_params=None, args=None):\n",
    "    # extract data from contentai.content_url\n",
    "    # or if needed locally use contentai.content_path\n",
    "    # after calling contentai.download_content()\n",
    "    print(\"Downloading content from ContentAI\")\n",
    "    contentai.download_content()\n",
    "    path_root = Path(__file__).parent\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"\"\"A script to perform model classification\"\"\",\n",
    "        epilog=\"\"\"\n",
    "        Launch with video parsing at a regular frame interval\n",
    "            python -u main.py --path_content a/video/file.mp4 --time_interval 3\n",
    "        Launch with moderation analysis for frames in a directory\n",
    "            python -u main.py --path_content a/frame --match_pattern \"*.jpg\"\n",
    "            ....\n",
    "    \"\"\", formatter_class=argparse.RawTextHelpFormatter)\n",
    "    submain = parser.add_argument_group('main execution and evaluation functionality')\n",
    "    submain.add_argument('--path_content', dest='path_content', type=str, default=contentai.content_path, \n",
    "                            help='input video path for files to label')\n",
    "    submain.add_argument('--path_result', dest='path_result', type=str, default=contentai.result_path, \n",
    "                            help='output path for samples')\n",
    "    submain.add_argument('--path_model', dest='path_model', type=str, \n",
    "                            default=str(path_root.joinpath('models', 'mobilenet_v2_140_224').resolve()),\n",
    "                            help='manifest path for model information')\n",
    "    submain.add_argument('--time_interval', dest='time_interval', type=float, default=1,  \n",
    "                            help='time interval for predictions from models')\n",
    "    submain.add_argument('--round_decimals', dest='round_decimals', type=int, default=5,  \n",
    "                            help='rounding decimals for predictions')\n",
    "    submain.add_argument('--match_pattern', dest='match_pattern', type=str, default=\"*.jpg\",  \n",
    "                            help='frame match pattern when running on an input directory')\n",
    "    submain.add_argument('--verbose', dest='verbose', default=False, action='store_true', \n",
    "                            help='verbosely print operations')\n",
    "\n",
    "    if args is not None:\n",
    "        config = vars(parser.parse_args(args))\n",
    "    else:\n",
    "        config = vars(parser.parse_args())\n",
    "    if input_params is not None:\n",
    "        config.update(input_params)\n",
    "    config.update(contentai.metadata())\n",
    "\n",
    "    print(f\"Run argments: {config}\")\n",
    "\n",
    "    path_output = Path(config['path_result'])\n",
    "    path_content = Path(config['path_content'])\n",
    "\n",
    "    # call process with i/o specified\n",
    "    print(f\"Processing input {config['path_content']} and writing to output dir '{path_output}'\")\n",
    "\n",
    "    model = {} # in a real example, you would load a model here...\n",
    "    if path_content.is_dir:\n",
    "        list_items = analyze_video(model, config['path_content'], config['time_interval'], config['verbose'], config['round_decimals'])\n",
    "    if len(list_items) == 0 and path_content.is_dir:\n",
    "        list_items = analyze_dir(model, config['path_content'], config['verbose'], config['match_pattern'], config['round_decimals'])\n",
    "\n",
    "    # write output of each class and segment\n",
    "    dict_result = {'config': {'version':\"1.0\", 'extractor':\"fancy_extractor\",\n",
    "                            'input':str(path_content.resolve()), 'timestamp': str(datetime.now()) }, \n",
    "                    'results':list_items }\n",
    "\n",
    "    if list_items is None or len(list_items) == 0:\n",
    "        print(f\"No predictions made from unique models...\")\n",
    "\n",
    "    elif len(config['path_result']) > 1:\n",
    "        if not path_output.exists():           # write out data if completed\n",
    "            path_output.mkdir(parents=True)\n",
    "\n",
    "        path_output = path_output.joinpath(\"data.json\")\n",
    "        with path_output.open('wt') as f:\n",
    "            json.dump(dict_result, f)\n",
    "        print(f\"Written to '{path_output.resolve()}'...\")\n",
    "\n",
    "\n",
    "    # return dict of data\n",
    "    if not contentai.running_in_contentai:\n",
    "        return dict_result\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #classify()\n",
    "    print(\"Disabled in notebook mode...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Deployment Material\n",
    "\n",
    "Nice job, that's all we wrote -- literally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
