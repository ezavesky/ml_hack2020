{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning Workshop: Content Insights 2020](assets/mlci_banner.jpg)\n",
    "\n",
    "# Machine Learning Workshop: Content Insights 2020\n",
    "\n",
    "Welcome to the workshop notebooks!  These notebooks are designed to give you a walk through the steps of creating a model, refining it with user labels, and testing it on content.  You can access the main [workshop forum page](https://INFO_SITE/forums/html/forum?id=241a0b77-7aa6-4fef-9f25-5ea351825725&ps=25), the [workshop files repo](https://INFO_SITE/communities/service/html/communityview?communityUuid=fb400868-b17c-44d8-8b63-b445d26a0be4#fullpageWidgetId=W403a0d6f86de_45aa_8b67_c52cf90fca16&folder=d8138bef-9182-4bdc-8b12-3c88158a219c), or the [symposium home page](https://software.web.DOMAIN) for additional help.\n",
    "\n",
    "The notebooks are divided into five core components: (A) setup & data, (B) model exploration, (C) labeling, (D) active labeling, (E) and deployment.  You are currently viewing the *setup & data* workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for running the workshop; we'll repeat these in the top line of each workbook.\n",
    "#   why repeat them? the backup routine only serializes .ipynb files, so others will need \n",
    "#   to be downloaded again if your compute instance restarts (a small price to pay, right?)\n",
    "\n",
    "WORKSHOP_BASE = \"https://vmlr-workshop.STORAGE\"\n",
    "AGG_AVFEAT = \"models/agg_avfeature.pkl.gz\"             # custom file for merged audio and video features\n",
    "CLASS_LABELS_FLAT = \"assets/labels_final.json\"  # provided file for label info\n",
    "CLASS_DEFINITIONS = \"assets/classes.json\"       # provided file for class info\n",
    "LEARNING_PERF_RERANK = \"assets/learning_rerank.pkl.gz\"\n",
    "LEARNING_PERF_QUERY = \"assets/learning_query.pkl.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook D: Active Labeling Analysis\n",
    "\n",
    "Now that we've reviewed **how** you can solicit labels and update the order of that solicitation, let's anlayze the implications of solicitation reording.  The late notebook focused on interacting with the labeling interface, so here we'll just use offline labels and simulate labeler entries.  Additionally, this notebook will focus on the training of a custom classifier instead of reusing other tags.\n",
    "\n",
    "In this notebook, we evaluate a few critical questions.\n",
    "\n",
    "1. Does reranking unlabeled instance (e.g. online learning) help to improve efficiency?\n",
    "1. What strategies for ordering results can improve labeling efficiency?\n",
    "1. What consensus measures should be taken for multiple labels?\n",
    "1. Are there trends in performance curves that can point to a moment of model stability?\n",
    "\n",
    "If you're really curious about the space, this overview paper, [A Wholistic View of Continual Learning with Deep Neural Networks: â€¨",
    "Forgotten Lessons and the Bridge to Active and Open World Learning](https://arxiv.org/abs/2009.01797), gives a great (and dizzying) review of active learning topics.\n",
    "\n",
    "![Machine Learning Workshop: Content Insights 2020](assets/active_overview.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Basics\n",
    "The cell below provides our basic training functions that are utilized in the notebook.  It is derived from the av-featuretraing method (classifier 3) evaluated in notebook B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a total of 1525 labels across 1000 samples and 6 classes.\n",
      "Loaded a total of 1033 samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>definition</th>\n",
       "      <th>primary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>holiday</td>\n",
       "      <td>holiday scenes or objects like decorated trees...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>halloween</td>\n",
       "      <td>halloween scenes where one or more characters ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gift giving</td>\n",
       "      <td>scenes of gift giving, receiving, or opening/u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>family moments</td>\n",
       "      <td>at least two people on screen, typically famil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shopping scenes</td>\n",
       "      <td>one or more primary actors in a store-like env...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class                                         definition  primary\n",
       "0          holiday  holiday scenes or objects like decorated trees...        1\n",
       "1        halloween  halloween scenes where one or more characters ...        1\n",
       "2      gift giving  scenes of gift giving, receiving, or opening/u...        1\n",
       "3   family moments  at least two people on screen, typically famil...        0\n",
       "4  shopping scenes  one or more primary actors in a store-like env...        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# define scorng functions\n",
    "def classifier_score(df_prediction, df_labels, class_name):\n",
    "    \"\"\"Functiont to provide metric outputs for the evaluation of a prediction dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        df_prediction (DataFrame): dataframe containing 'asset' and 'score' as columns\n",
    "        df_labels (DataFrame): dataframe containing 'asset' and 'class' for labels\n",
    "        class_name (str): class name for evaluation against labels\n",
    "\n",
    "    Returns:\n",
    "        dict of metrics (AUC, AP, precision, recall) ({\"ap\":X, \"class\":Y, ...}) and joined dataframe\n",
    "    \"\"\"\n",
    "    metrics_obj = {\"class\":class_name}\n",
    "    \n",
    "    # clean up input labels, prune to relevant class\n",
    "    df_join = df_prediction\n",
    "    if \"class\" not in df_prediction.columns:\n",
    "        df_labels = df_labels[df_labels[\"class\"] == class_name].drop(columns=[\"etag\", \"url\"]) \n",
    "        # join labels and scores by asset, nomalize score to float\n",
    "        df_join = df_prediction.set_index('asset').join(df_labels.set_index('asset'), how=\"left\").fillna(0)  # joint at asset level, 0 for nonscoring\n",
    "        df_join[\"class\"] = df_join[\"class\"].apply(lambda x: 1 if x != 0 else 0).astype(int)\n",
    "        df_join = df_join.reset_index().sort_values(\"score\", ascending=False)\n",
    "\n",
    "    # print(f\"{class_name}: Found {len(df_join)} samples from {len(df_labels)} labels and {len(df_prediction)} scores.\")\n",
    "\n",
    "    def thresh(x):\n",
    "        return 1 if x >= 0.5 else 0\n",
    "    \n",
    "    metrics_obj[\"AP\"] = metrics.average_precision_score(df_join['class'], df_join['score'])\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(df_join['class'], df_join['score'])\n",
    "    metrics_obj[\"AUC\"] = metrics.auc(fpr, tpr)\n",
    "    metrics_obj[\"Accuracy\"] = metrics.accuracy_score(df_join['class'], df_join['score'].apply(thresh))\n",
    "    _, metrics_obj[\"Recall\"], metrics_obj[\"F1\"], _ = metrics.precision_recall_fscore_support(\n",
    "        df_join['class'], df_join['score'].apply(thresh), average='macro', warn_for=())\n",
    "    #metrics_obj[\"Recall\"] = metrics.recall_score(df_join['class'], df_join['score'].apply(thresh))\n",
    "    #metrics_obj[\"F1\"] = metrics.f1_score(df_join['class'], df_join['score'].apply(thresh))\n",
    "    # print(f\"{class_name}: {metrics_obj}\")\n",
    "        \n",
    "    # return our computation!\n",
    "    return metrics_obj, df_join\n",
    "\n",
    "def classifier_plot(metrics_obj, df_scored):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(df_scored['class'], df_scored['score'])\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "    lw = 2\n",
    "    ax1.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label=f\"AUC curve (area={metrics_obj['AUC']:0.2})\")\n",
    "    ax1.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(df_scored['class'], df_scored['score'])\n",
    "    ax2.plot(recall, precision, color='red',\n",
    "             lw=lw, label=f\"PR Curve (AP={metrics_obj['AP']:0.2}, F1={metrics_obj['F1']:0.2})\")\n",
    "    ax2.plot([1, 0], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "    \n",
    "# read label data for use later!\n",
    "df_labels = pd.read_json(CLASS_LABELS_FLAT).explode('labels').fillna('none of the above')\n",
    "df_labels.rename(columns={\"data\":\"url\", \"labels\":\"class\"}, inplace=True)\n",
    "df_labels[\"asset\"] = df_labels['url'].replace(regex={r'^' + WORKSHOP_BASE + '/': ''})\n",
    "print(f\"Loaded a total of {len(df_labels)} labels across {len(df_labels['asset'].unique())} samples and {len(df_labels['class'].unique())} classes.\")\n",
    "\n",
    "# clear out other performance stores\n",
    "df_performance = None\n",
    "\n",
    "# load features\n",
    "path_features = Path(AGG_AVFEAT)\n",
    "if not path_features.exists():\n",
    "    raise Exception(f\"\"\"\n",
    "       Sorry, the set of aggregate features was not found.  \n",
    "       Please return to notebook B to create file '{str(path_features)}'...\n",
    "    \"\"\")\n",
    "df_avfeature = pd.read_pickle(str(path_features))\n",
    "print(f\"Loaded a total of {len(df_avfeature)} samples.\")\n",
    "\n",
    "df_classes = pd.read_json(CLASS_DEFINITIONS)\n",
    "display(df_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut to Learning\n",
    "Disable the below cell (change `True` to `False`) to perform a full re-run of simulations.  Yes, it's a little bit of a cheat, but the numbers should be the same wherever they are run (*we use a constant random number generator seed*), so there's no shame in taking this shortcut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_rerank = None  # variables that are defined later\n",
    "df_query_sizes = None\n",
    "\n",
    "if True:\n",
    "    df_perf_rerank = pd.from_pickle(LEARNING_PERF_RERANK)\n",
    "    df_query_sizes = pd.from_pickle(LEARNING_PERF_QUERY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking to Improve Efficency\n",
    "One major advantage of active learning is truncating the time needed to label a highly performant model.  We'll explore four strategies on this dataset to demonstrate the power of active learning.  For a more in-depth read (and in-depth methods), we suggest consultation of the paper mentioned at the top of this notebook.\n",
    "\n",
    "1. **Random** query - random sample of remaining items\n",
    "2. Most outlying, dissimilar tasks first - learn a classifier and pick from the most confusing (e.g. on the decision boundary)\n",
    "    1. **Outlier** - Scores that are the most divergent from their label\n",
    "    2. **Anomalies** - looking for singleton outlier to target first\n",
    "    3. **Entropy** - compute the entropy of the training set and look for highest entropy additions\n",
    "    4. **Local Outliers** - using a kNN appreach, find the most disconnected samples\n",
    "3. Most inlying, similar tasks first - learn a classifier and pick from the top scoring results\n",
    "    1. **Inlier** - Scores agree the most from their label (positive or negative)\n",
    "    2. **Positive** - sample from the high scoring items of the unlabeled (may lead to lower recall)\n",
    "    3. **Negative** - sample form the low scoring items of the unlabeled (may lead ot better diversity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Sizes: [5, 5, 10, 10, 20, 20, 40, 40, 80, 80, 160, 160, 320, 320]\n",
      "fold: 0, class: holiday, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 1, class: holiday, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 2, class: holiday, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 3, class: holiday, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 4, class: holiday, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 0, class: halloween, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 1, class: halloween, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 2, class: halloween, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 3, class: halloween, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "fold: 4, class: halloween, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 0, class: gift giving, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 1, class: gift giving, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 2, class: gift giving, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "fold: 3, class: gift giving, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 4, class: gift giving, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 0, class: family moments, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 1, class: family moments, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 2, class: family moments, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 3, class: family moments, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 4, class: family moments, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 0, class: shopping scenes, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 1, class: shopping scenes, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 2, class: shopping scenes, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "Warning, insufficient class diversity with 5 samples..\n",
      "Warning, insufficient class diversity with 10 samples..\n",
      "fold: 3, class: shopping scenes, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "fold: 4, class: shopping scenes, methods: ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative']\n",
      "Computed impact by reranking strategy.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def classify_av(methods=[], label_increment=5, query_sizes=[]):\n",
    "    global df_labels\n",
    "    cv_folds = 5\n",
    "    feature = \"combined\"\n",
    "    score_calibrate = False\n",
    "    cv_jobs = -1  # -1 is auto, otherwise specific number\n",
    "    if not methods:\n",
    "        methods = ['anomaly', 'entropy', 'random', 'outlier', 'inlier', 'positive', 'negative'] \n",
    "        #, 'localoutlier'] - almost always similar (but worse) to anomaly\n",
    "    \n",
    "    label_len = label_increment\n",
    "    total_len = len(df_labels[\"asset\"].unique())\n",
    "    if not query_sizes:   # should we compute label size?\n",
    "        total_len_eval = total_len\n",
    "        while total_len_eval > 0:   # consume chunks of labels: 5, 5, 10, 10, 20, 20, 40, 40, ...\n",
    "            query_sizes.append(label_len)\n",
    "            total_len_eval -= label_len\n",
    "            query_sizes.append(label_len)\n",
    "            total_len_eval -= label_len\n",
    "            label_len *= 2\n",
    "    print(f\"Query Sizes: {query_sizes}\")\n",
    "\n",
    "    list_res_intermediate = []\n",
    "    for idx in range(len(df_classes)):  # iterate classes for evaluation\n",
    "        row = df_classes.iloc[idx]\n",
    "        \n",
    "        df_label_sub = df_labels[df_labels[\"class\"]==row[\"class\"]]  # subselect for this class\n",
    "        df_feat = df_avfeature.set_index(\"asset\").copy()   # get slice of right features\n",
    "        df_feat = df_feat.join(df_label_sub.set_index(\"asset\"), how=\"left\").fillna(0)  # join with labels\n",
    "        df_feat[\"class\"] = df_feat[\"class\"].apply(lambda x: 1 if x != 0 else 0).astype(int)  # blank out text\n",
    "\n",
    "        idx_asset = df_feat.index\n",
    "        idx_numeric = list(range(len(idx_asset)))  # pull index from features\n",
    "        rng = np.random.RandomState(0)\n",
    "        \n",
    "        for n_fold in range(cv_folds):   # our cross-validation folds\n",
    "            rng.shuffle(idx_numeric)\n",
    "\n",
    "            X_feat = np.vstack(df_feat[feature])\n",
    "            y_label = df_feat[\"class\"].values\n",
    "            #print(\"INITIAL\", len(idx_asset), len(X_feat), X_feat.shape, y_label.shape)\n",
    "\n",
    "            for method_act in methods:   # iterate through all sampling methods\n",
    "                idx_seen = []\n",
    "                idx_unseen = idx_numeric.copy()\n",
    "                \n",
    "                for query_idx in range(len(query_sizes)):   # how many labels can we see in thish round?\n",
    "                    query_len = query_sizes[query_idx]\n",
    "                    if query_len > len(idx_unseen):   # went over the count break now?\n",
    "                        break\n",
    "                    idx_seen += idx_unseen[:query_len]   # add to labels we can see\n",
    "                    idx_unseen = idx_unseen[query_len:]  # remove from available set                    \n",
    "                    \n",
    "                    model = LogisticRegression()  # basic logistic regression\n",
    "                    if score_calibrate:   # try to re-calibrate outputs for better threshold?\n",
    "                        model = CalibratedClassifierCV(model, method=\"sigmoid\")\n",
    "                    \n",
    "                    X_train = X_feat[idx_seen, :]\n",
    "                    y_train = y_label[idx_seen]\n",
    "                    if len(np.unique(y_train)) < 2:\n",
    "                        print(f\"Warning, insufficient class diversity with {len(y_train)} samples..\")\n",
    "                        continue\n",
    "                    model.fit(X_train, y_train)   # retrain on this segment\n",
    "                    \n",
    "                    X_test = X_feat[idx_unseen,:]\n",
    "                    probs = model.predict_proba(X_test)   # predict for all samples\n",
    "                    scores = probs[:,1]\n",
    "                    y_test = y_label[idx_unseen]\n",
    "\n",
    "                    \n",
    "                    # last time we trained over everything via cross-fold\n",
    "                    scores_train = None\n",
    "                    if False:   # doubles training time a least!\n",
    "                        try:\n",
    "                            probs_train = cross_val_predict(model, X_train, y_train, cv=2, \n",
    "                                                          n_jobs=cv_jobs, method='predict_proba')\n",
    "                            scores_train = probs_train[:,1]\n",
    "                        except ValueError as e:\n",
    "                            pass\n",
    "\n",
    "                    if method_act == \"inlier\":   # inlier is most confident (most similar to label)\n",
    "                        delta_val = np.absolute(scores - y_test)\n",
    "                        idx_resort = np.argsort(-delta_val)\n",
    "                        idx_unseen[:] = [idx_unseen[i] for i in idx_resort]  # reorder by priority\n",
    "                        \n",
    "                    elif method_act == \"outlier\":   # outlier is least confident (divergent from label)\n",
    "                        delta_val = np.absolute(scores - y_test)\n",
    "                        idx_resort = np.argsort(delta_val)\n",
    "                        #print(idx_resort, len(idx_resort), idx_resort.min(), idx_resort.max())\n",
    "                        #print(len(idx_unseen))\n",
    "                        idx_unseen[:] = [idx_unseen[i] for i in idx_resort]  # reorder by priority\n",
    "                        \n",
    "                    elif method_act == \"positive\":   # grab the top of the ranked list\n",
    "                        idx_resort = np.argsort(-scores)\n",
    "                        idx_unseen[:] = [idx_unseen[i] for i in idx_resort]  # reorder by priority\n",
    "\n",
    "                    elif method_act == \"negative\":   # grab the top of the ranked list\n",
    "                        idx_resort = np.argsort(scores)\n",
    "                        idx_unseen[:] = [idx_unseen[i] for i in idx_resort]  # reorder by priority    \n",
    "                        \n",
    "                    elif method_act == \"anomaly\":   # find outliers from the unlabeled set\n",
    "                        # See this page for more details...\n",
    "                        #.  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest.score_samples\n",
    "                        outlier_model = IsolationForest(random_state=0).fit(X_train)\n",
    "                        anomaly_score = outlier_model.score_samples(X_test)\n",
    "                        idx_resort = np.argsort(anomaly_score)\n",
    "                        idx_unseen[:] = [idx_unseen[i] for i in idx_resort]  # reorder by priority\n",
    "\n",
    "                    elif method_act == \"localoutlier\":   # find outliers from the unlabeled set\n",
    "                        # See this page for more details...\n",
    "                        #.  https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor\n",
    "                        # Similar, but almost always worse than `anomaly`\n",
    "                        outlier_model = LocalOutlierFactor(n_neighbors=5, novelty=True).fit(X_train)\n",
    "                        anomaly_score = outlier_model.score_samples(X_test)\n",
    "                        idx_resort = np.argsort(anomaly_score)\n",
    "                        idx_unseen[:] = [idx_unseen[i] for i in idx_resort]  # reorder by priority    \n",
    "                        \n",
    "                    elif method_act == \"entropy\":   # look for least simialr in feature space\n",
    "                        # NOTE: This code block is copied from scikit documentation\n",
    "                        # https://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_digits_active_learning.html?highlight=active\n",
    "                        lp_model = LabelSpreading(gamma=0.25, max_iter=20)\n",
    "                        y_unknown = np.copy(y_label)\n",
    "                        y_unknown[idx_unseen] = -1   # add new \"unknown\" class\n",
    "                        lp_model.fit(X_feat, y_unknown)\n",
    "                        \n",
    "                        # compute the entropies of transduced label distributions\n",
    "                        pred_entropies = stats.distributions.entropy(\n",
    "                            lp_model.label_distributions_.T)\n",
    "                        pred_entropies_sub = pred_entropies[idx_unseen]\n",
    "                        #print(pred_entropies, pred_entropies.shape)\n",
    "\n",
    "                        # select up to 5 digit examples that the classifier is most uncertain about\n",
    "                        uncertainty_index = np.argsort(pred_entropies_sub)[::-1]\n",
    "                        #print(len(idx_unseen), uncertainty_index.shape, pred_entropies_sub.shape)\n",
    "                        idx_unseen[:] = [idx_unseen[i] for i in uncertainty_index if i in idx_unseen]  # reorder by priority\n",
    "                        #uncertainty_index = uncertainty_index[\n",
    "                        #    np.in1d(uncertainty_index, unlabeled_indices)][:5]\n",
    "\n",
    "                    with warnings.catch_warnings():\n",
    "                        # NOTE: we get some nasty warnings because sampling can't always guarantee classes\n",
    "                        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                        warnings.simplefilter(\"ignore\", category=UndefinedMetricWarning)\n",
    "                        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "                        warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "                        \n",
    "                        metrics_obj, _ = classifier_score(\n",
    "                            pd.DataFrame({\"score\":scores, \"class\":y_test}), df_feat[\"class\"], row['class'])\n",
    "                        if scores_train is not None:  # had enough data in train set?\n",
    "                            metrics_train, _ = classifier_score(\n",
    "                                pd.DataFrame({\"score\":scores_train, \"class\":y_train}), df_feat[\"class\"], row['class'])\n",
    "                            metrics_obj[\"AP_train\"] = metrics_train[\"AP\"]\n",
    "                            metrics_obj[\"Recall_train\"] = metrics_train[\"Recall\"]\n",
    "                            metrics_obj[\"AUC_train\"] = metrics_train[\"AUC\"]\n",
    "                        \n",
    "                    # too many iterations here, discard the scores\n",
    "                    metrics_obj.update({'class':row['class'], 'method':method_act, \n",
    "                                        \"size_train\":len(idx_seen), \"size_test\":len(idx_unseen),\n",
    "                                        \"query_idx\":query_idx, \"label_increment\":label_increment})\n",
    "                    list_res_intermediate.append(metrics_obj)\n",
    "            print(f\"fold: {n_fold}, class: {row['class']}, methods: {methods}\")\n",
    "    # end loop over everything\n",
    "    df_intermediate = pd.DataFrame(list_res_intermediate)\n",
    "    return df_intermediate\n",
    "\n",
    "# run classifier for overall sampling\n",
    "if df_perf_rerank is None:\n",
    "    df_perf_rerank = classify_av()\n",
    "    df_perf_rerank.to_pickle(LEARNING_PERF_RERANK)\n",
    "print(\"Computed impact by reranking strategy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97f2040b7574ce087cd61b02a429dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Metric:', options=('AP', 'AUC', 'Accuracy'), value='AP')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fee6b33174464f8e5cd26206d076bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "\n",
    "def active_perf_visualize(f, df, title_overall, metric_name, class_name=None):\n",
    "    df_perf_avg = df.groupby(['method', 'class', 'size_train', 'label_increment']).mean().reset_index()\n",
    "    if class_name is not None:\n",
    "        df_perf_avg = df_perf_avg[df_perf_avg[\"class\"]==class_name]\n",
    "    lw = 2\n",
    "    method_list = list(df_perf_avg['method'].unique())\n",
    "    share_axy = None\n",
    "    f.clf()\n",
    "    def plt_local(ax, grp_class):\n",
    "        for idx_method, grp_method in grp_class.groupby('method'):\n",
    "            queryK = grp_method[\"size_train\"]\n",
    "            resultAP = grp_method[metric_name]\n",
    "            ax.plot(queryK, resultAP, lw=lw, label=f\"{idx_method} ({metric_name})\")\n",
    "        #ax.set_prop_cycle(None) # color reset - https://stackoverflow.com/a/24283087\n",
    "        #for idx_method, grp_method in grp_class.groupby('method'):\n",
    "        #    queryK = grp_method[\"size_train\"]\n",
    "        #    resultAP = grp_method[\"AUC\"]\n",
    "        #    ax.plot(queryK, resultAP, lw=lw, linestyle=\"--\", label=f\"{idx_method} (AUC)\")\n",
    "    \n",
    "    idx_graph = 1\n",
    "    for idx_class, grp_class in df_perf_avg.groupby(['class', 'label_increment']):\n",
    "        ax = f.add_subplot(3, 3, idx_graph, sharey=share_axy)\n",
    "        plt_local(ax, grp_class)\n",
    "        if idx_graph > 3:\n",
    "            ax.set_xlabel('train size')\n",
    "        ax.grid()\n",
    "        ax.set_title(f\"{idx_class[0]} @ Query {idx_class[1]}\")\n",
    "        if share_axy is None:\n",
    "            share_axy = ax\n",
    "            # ax.set_ylabel('mAP (mean average precision)')\n",
    "        # ax.legend(loc=\"best\")\n",
    "        idx_graph += 1\n",
    "    # one more graph that we'll hide most of \n",
    "    ax = f.add_subplot(3, 3, idx_graph)\n",
    "    plt_local(ax, grp_class)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_axis_off()\n",
    "    ax.legend(loc=\"upper left\") #, ncol=2)\n",
    "    ax.set_title(title_overall)\n",
    "    display(f)\n",
    "\n",
    "\n",
    "# use widget interaction basic\n",
    "f = plt.figure(figsize=(12,12))\n",
    "\n",
    "demo_fn = partial(active_perf_visualize, f, df_perf_rerank, \"Average Precision on Unseen Samples\")\n",
    "dropdown_metric = widgets.Dropdown(\n",
    "    options=[\"AP\", \"AUC\", \"Accuracy\"], \n",
    "    value=\"AP\",  # send run names\n",
    "    description='Metric:',\n",
    "    disabled=False\n",
    ")\n",
    "out = widgets.interactive_output(demo_fn, {\"metric_name\":dropdown_metric})\n",
    "# output = dropdown.children[-1]  # anti-flicker trick (https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html#Flickering-and-jumping-output)\n",
    "# output.layout.height = '750px'  # disable this if you make your output window longer!\n",
    "display(dropdown_metric, out)\n",
    "\n",
    "# active_perf_visualize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking Post-Mortem\n",
    "Whoa, there's a lot to unpack in the graphs above, so let's just iterate directly!  Overall there are a few winning methods that we'll carry into our next analysis.  Remember that these scores are computed on the `unknown` set of samples, so the size of that set decreases over time.\n",
    "1. Random does okay, but flatlines\n",
    "2. The simpler, prediction score-based methods (`positive`, `negative`) don't do well overall; in fact, we see that because of increasing homogeniety in `positive`, the performance drops with more samples.\n",
    "3. In these experiments, the training sizes between 200-300 worked best (appx 20-30% of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Depth Experiments\n",
    "\n",
    "Now that we've got some better performance statistics, let's benchmark the effect of bigger or smaller query sizes.  Specifically, this indicates how often we may need to retrain and update scores in our labeling system.  \n",
    "\n",
    "In one school of thought, the thought is that smalll, micro-updates from the user will allow the machine learning model to quickly adapt.  However, keep in mind that the `random` baseline above (just random ordering of samples) was quite performant among all of the different classes. \n",
    "\n",
    "In another practice, recent studies have found to \"go big\" on the first initialization of a classifier, as applied in [Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](https://arxiv.org/abs/2002.11794).  This choice asserts that by learning over. alarge sample first, the diversity of those samples is better guaranteed.  After a large model is created, the thought is that additional transfer learning and adaptation strategies can move forward to exploit and tune the model for various sample space variances.\n",
    "\n",
    "We won't delve into those advanced techniques here, but we can experiment to emulate various query depths that can be utilized in a labeling sysystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Sizes: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "fold: 0, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "Query Sizes: [50, 50, 50, 50, 50, 50, 50, 50]\n",
      "fold: 0, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "Query Sizes: [100, 100, 100, 100]\n",
      "fold: 0, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: holiday, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: halloween, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: gift giving, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: family moments, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 0, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 1, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 2, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 3, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "fold: 4, class: shopping scenes, methods: ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
      "Computed impact by query (audience solicitation) size.\n"
     ]
    }
   ],
   "source": [
    "# run query size estimation\n",
    "if df_query_sizes is None:\n",
    "    list_methods = ['anomaly', 'inlier', 'outlier', 'random', 'negative']\n",
    "    query_sizes = [20] *20\n",
    "    df_query_sizes = classify_av(methods=list_methods, \n",
    "                                 label_increment=20, query_sizes=query_sizes)\n",
    "    query_sizes = [50] * 8\n",
    "    df_query_sizes = df_query_sizes.append(classify_av(methods=list_methods, \n",
    "                                 label_increment=50, query_sizes=query_sizes))\n",
    "    query_sizes = [100] * 4\n",
    "    df_query_sizes = df_query_sizes.append(classify_av(methods=list_methods, \n",
    "                                 label_increment=100, query_sizes=query_sizes))\n",
    "    df_query_sizes.to_pickle(LEARNING_PERF_QUERY)\n",
    "print(\"Computed impact by query (audience solicitation) size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x864 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17da86031a94485b1d921a82aa52c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Class:', options=('holiday', 'halloween', 'gift giving', 'family moments', 'shopping sceâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3bc0bc30d049078671a818f1bb5b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Metric:', index=1, options=('AP', 'AUC', 'Accuracy'), value='AUC')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dafc79850fa489d91125b7ef3a40f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use widget interaction basic\n",
    "f = plt.figure(figsize=(12,12))\n",
    "demo_fn = partial(active_perf_visualize, f, df_query_sizes, \"Average Precision Query Size Effect\")\n",
    "dropdown_class = widgets.Dropdown(\n",
    "    options=list(df_query_sizes['class'].unique()),  # send run names\n",
    "    value=list(df_query_sizes['class'].unique())[0],  # send run names\n",
    "    description='Class:',\n",
    "    disabled=False\n",
    ")\n",
    "out = widgets.interactive_output(demo_fn, {'metric_name':dropdown_metric, 'class_name':dropdown_class})\n",
    "display(dropdown_class, dropdown_metric, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Depth Post-Mortem\n",
    "A few messages come out of this experiment set that mostly agree with recent publications: start big and then hone-in quickly with small tuning.\n",
    "1. The average precision (AP) of small depth query sampling worked well for the `inlier` method, but only after a quarter  fo the dataset (or half of this experiment) was already confirmed. \n",
    "2. All models (except `inliner`) did well on \"big starts\" where a random set of as many as 100 samples were labeled before any reranking occured.\n",
    "3. Perhaps most telling of a potential weakness, the `inlier` method shows dramatic oscillations when used early in sampling and with small query depths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consensus Measures\n",
    "Consensus is an important factor for determining agreement among labelers.  Few labeling systems properly use consensus to determine accuracy of an underlying label because the methods for establishing agreement among labelers relies both on the labels themselves as well as an individual's performance record.  The correct aggregation and balance of these two strategies could be a hackathon or workshop in its own (and arguably was in such datasets as the [Netflix Prize](https://www.kaggle.com/netflix-inc/netflix-prize-data)) because a variety of ETL and data clean-up processs are required.\n",
    "\n",
    "Unfortunately, at the time of writing, there weren't enough labels on this dataset, so we could not conduct an in-depth consensus analysis.  Never fear! Parallel work on other tracks in [LabelQuest](https://lq.web.DOMAIN) come and go as requied by business, so future datasets will provide sufficient means for exploration.\n",
    "\n",
    "![Counts of Label Overlap](assets/active_consensus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Active Learning Material\n",
    "\n",
    "This is where the core technical evaluations end, congratulations -- you made it!  Armed with this knowledge, you have a few strategies to map from a new concept into a custom av-centric classifier with several evaluation metrics along the way.  \n",
    "\n",
    "The next notebook, [notebook E](E_deployment.ipynb) *(that link may not work)* visits advanced methods that can apply and utilize models generated from these work books.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
