{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning Workshop: Content Insights 2020](assets/mlci_banner.jpg)\n",
    "\n",
    "# Machine Learning Workshop: Content Insights 2020\n",
    "\n",
    "Welcome to the workshop notebooks!  These notebooks are designed to give you a walk through the steps of creating a model, refining it with user labels, and testing it on content.  You can access the main [workshop forum page](https://INFO_SITE/forums/html/forum?id=241a0b77-7aa6-4fef-9f25-5ea351825725&ps=25), the [workshop files repo](https://INFO_SITE/communities/service/html/communityview?communityUuid=fb400868-b17c-44d8-8b63-b445d26a0be4#fullpageWidgetId=W403a0d6f86de_45aa_8b67_c52cf90fca16&folder=d8138bef-9182-4bdc-8b12-3c88158a219c), or the [symposium home page](https://software.web.DOMAIN) for additional help.\n",
    "\n",
    "The notebooks are divided into five core components: (A) setup & data, (B) model exploration, (C) labeling, (D) active labeling, (E) and deployment.  You are currently viewing the *setup & data* workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for running the workshop; we'll repeat these in the top line of each workbook.\n",
    "#   why repeat them? the backup routine only serializes .ipynb files, so others will need \n",
    "#   to be downloaded again if your compute instance restarts (a small price to pay, right?)\n",
    "\n",
    "WORKSHOP_BASE = \"https://vmlr-workshop.STORAGE\"\n",
    "# WORKSHOP_BASE = \"http://content.research.DOMAIN/projects/mlci_2020\"\n",
    "AGG_METADATA = \"agg_metadata.pkl.gz\"            # custom file for merged metadata\n",
    "CLASS_DEFINITIONS = \"assets/classes.json\"       # provided file for class info\n",
    "CLASS_LABELS_FLAT = \"assets/labels_final.json\"  # provided file for label info\n",
    "AGG_TAG_EMBEDDING = \"agg_tag_vocab.w2v\"         # custom file for tag-based vocabulary\n",
    "AGG_LABELS = \"agg_labels.pkl.gz\"                # custom file for merged labels\n",
    "AGG_AVFEAT = \"agg_avfeature.pkl.gz\"             # custom file for merged audio and video features\n",
    "MODEL_PERFORMANCE = \"model_performance.pkl.gz\"  # custom file for storing model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook B: Building Models for Contextual Ads\n",
    "\n",
    "One goal of combining timed metadata and content is for better alignment of content from WarnerMedia and ads from Xandr creatives.  The image below demonstrates one example where a detected keyword or scene can trigger an ad that is realted.  Without this technology, this ad spot (or inventory) may be undersold and filled with an unrelated or standard campaign ad.\n",
    "\n",
    "![Contextual Ad Product](assets/mlci_contextual.jpg)\n",
    "\n",
    "## Class Exploration\n",
    "Let's quickly load and display the target classes used in this experiment. The table below indicates the class and the definition utilized for our classifier.  The field `primary` indicates whether or not a class will be used for performance evaluations.  Some non-primary classes were also included for additional experimentation, but they will not be the focus here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>definition</th>\n",
       "      <th>primary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>holiday</td>\n",
       "      <td>holiday scenes or objects like decorated trees, presents, or character, holiday party</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>halloween</td>\n",
       "      <td>halloween scenes where one or more characters are in costume, ideally one or more characters are trick-or-treating</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gift giving</td>\n",
       "      <td>scenes of gift giving, receiving, or opening/unwrapping</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>family moments</td>\n",
       "      <td>at least two people on screen, typically familes at parties, enjoying a meal, lounging at home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shopping scenes</td>\n",
       "      <td>one or more primary actors in a store-like environment; not necessary to see their face</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class  \\\n",
       "0          holiday   \n",
       "1        halloween   \n",
       "2      gift giving   \n",
       "3   family moments   \n",
       "4  shopping scenes   \n",
       "\n",
       "                                                                                                           definition  \\\n",
       "0                               holiday scenes or objects like decorated trees, presents, or character, holiday party   \n",
       "1  halloween scenes where one or more characters are in costume, ideally one or more characters are trick-or-treating   \n",
       "2                                                             scenes of gift giving, receiving, or opening/unwrapping   \n",
       "3                      at least two people on screen, typically familes at parties, enjoying a meal, lounging at home   \n",
       "4                             one or more primary actors in a store-like environment; not necessary to see their face   \n",
       "\n",
       "   primary  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 classes and 122736 tag rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "df_classes = pd.read_json(CLASS_DEFINITIONS)\n",
    "display(df_classes)\n",
    "\n",
    "path_metadata = Path(AGG_METADATA)\n",
    "if not path_metadata.exists():\n",
    "    print(\"Please return to notebook A to run data flattening and merging!\")\n",
    "df_flatten = pd.read_pickle(str(path_metadata))\n",
    "print(f\"Loaded {len(df_classes)} classes and {len(df_flatten)} tag rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Browsing for Subjective Scoring\n",
    "Below, we've created a simple method to view keyframe results from the video assets used in this workshop.  It uses simple widgets (as we did above) and displays them in a columnar form in ths notebook.  In addition to numerical performance (accuracy, AUC, etc.), we can visually inspect the performance of a classifier model with this utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a714a546e541af8fe5568accae9d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButton(value=False, description='Randomize'), Output()), _dom_classes=('widget-intâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.regen(x)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def display_hit_html(path_result, score, label=1, idx=None, base=WORKSHOP_BASE):\n",
    "    '''\n",
    "     This function essentially convert the image url to \n",
    "     '<img src=\"'+ path + '\"/>' format. And one can put any\n",
    "     formatting adjustments to control the height, aspect ratio, size etc.\n",
    "     within as in the below example. \n",
    "    '''\n",
    "    path_parts = path_result.split('/')\n",
    "    url_full = f\"{base}/{path_parts[0]}/keyframe/{path_parts[1]}.jpg\"\n",
    "    text_color = f\" style='color:{'black' if label==1 else 'red'}' \"\n",
    "    return f\"\"\"\n",
    "        <a href='{url_full}' title='{path_result}' target='_new'><img width='100%' src='{url_full}' /></a>\n",
    "        <small {text_color}><strong>{f\"[{idx}] \" if idx is not None else ''}{score:0.4}</strong></small>\n",
    "        \"\"\"\n",
    "\n",
    "def results_display(df, max_results=32, num_cols=8, base=WORKSHOP_BASE, title=\"Example Results\"):\n",
    "    \"\"\"Input DataFrame with 'asset' and 'score' and show those results in columnar format.\n",
    "    Providing an additional column 'label' [as 0 or 1] will allow incorrect items to be shown in red.\"\"\"\n",
    "    width_col = f\"{round(100/num_cols)}%\"\n",
    "    #list_results.sort(reverse=True, key=lambda x: x[-1]) \n",
    "    #list_html = [widgets.HTML(path_to_image_html(*x, base=base)) for x in list_results]\n",
    "    df = df.sort_values(\"score\", ascending=False).head(max_results).reset_index(drop=True)\n",
    "    if \"class\" not in df.columns:\n",
    "        df[\"class\"] = 1\n",
    "    list_html = [widgets.HTML(display_hit_html(\n",
    "        x['asset'], x['score'], x['class'], idx=i, base=base)) for i,x in df.iterrows()]\n",
    "    display(widgets.VBox([widgets.HTML(f\"<h3>{title}</h3>\"), widgets.GridBox(list_html,\n",
    "               layout=widgets.Layout(grid_template_columns=f\"repeat(8, {width_col})\"))]))\n",
    "\n",
    "# demo of input as a single result, but with random scores\n",
    "def regen(x):\n",
    "    df_demo = pd.DataFrame({\"asset\":list(df_flatten['asset'].sample(16)),\n",
    "                            \"score\":np.random.rand(1, 16)[0],\n",
    "                            \"class\":np.random.randint(2, size=16)})\n",
    "    results_display(df_demo, title=\"Results demo (random images, scores, and labels utilized)\")\n",
    "widgets.interact(regen, x=widgets.ToggleButton(description=\"Randomize\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Result Scoring\n",
    "Similar to the code above, this function can be used for evaluation of models by a few different metrics -- but here it is aimed at objective, numerical methods.  For convenience, this method can also plot the results of both objective and subjective scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a total of 947 labels across 589 samples and 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def classifier_score(df_prediction, df_labels, class_name):\n",
    "    \"\"\"Functiont to provide metric outputs for the evaluation of a prediction dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        df_prediction (DataFrame): dataframe containing 'asset' and 'score' as columns\n",
    "        df_labels (DataFrame): dataframe containing 'asset' and 'label' for labels\n",
    "        class_name (str): class name for evaluation against labels\n",
    "\n",
    "    Returns:\n",
    "        dict of metrics (AUC, AP, precision, recall) ({\"ap\":X, \"class\":Y, ...}) and joined dataframe\n",
    "    \"\"\"\n",
    "    metrics_obj = {\"class\":class_name}\n",
    "    \n",
    "    # clean up input labels, prune to relevant class\n",
    "    df_labels = df_labels[df_labels[\"class\"] == class_name].drop(columns=[\"etag\", \"url\"]) \n",
    "    # join labels and scores by asset, nomalize score to float\n",
    "    df_join = df_prediction.set_index('asset').join(df_labels.set_index('asset'), how=\"left\").fillna(0)  # joint at asset level, 0 for nonscoring\n",
    "    df_join[\"class\"] = df_join[\"class\"].apply(lambda x: 1 if x != 0 else 0).astype(int)\n",
    "    df_join = df_join.reset_index().sort_values(\"score\", ascending=False)\n",
    "\n",
    "    # print(f\"{class_name}: Found {len(df_join)} samples from {len(df_labels)} labels and {len(df_prediction)} scores.\")\n",
    "\n",
    "    def thresh(x):\n",
    "        return 1 if x >= 0.5 else 0\n",
    "    \n",
    "    metrics_obj[\"AP\"] = metrics.average_precision_score(df_join['class'], df_join['score'])\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(df_join['class'], df_join['score'])\n",
    "    metrics_obj[\"AUC\"] = metrics.auc(fpr, tpr)\n",
    "    metrics_obj[\"Accuracy\"] = metrics.accuracy_score(df_join['class'], df_join['score'].apply(thresh))\n",
    "    metrics_obj[\"Recall\"] = metrics.recall_score(df_join['class'], df_join['score'].apply(thresh))\n",
    "    metrics_obj[\"F1\"] = metrics.f1_score(df_join['class'], df_join['score'].apply(thresh))\n",
    "    # print(f\"{class_name}: {metrics_obj}\")\n",
    "        \n",
    "    # return our computation!\n",
    "    return metrics_obj, df_join\n",
    "\n",
    "def classifier_plot(metrics_obj, df_scored):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(df_scored['class'], df_scored['score'])\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "    lw = 2\n",
    "    ax1.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label=f\"AUC curve (area = {metrics_obj['AUC']:0.2})\")\n",
    "    ax1.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(df_scored['class'], df_scored['score'])\n",
    "    ax2.plot(recall, precision, color='red',\n",
    "             lw=lw, label=f\"PR Curve (F1 = {metrics_obj['F1']:0.2})\")\n",
    "    ax2.plot([1, 0], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "    \n",
    "# read label data for use later!\n",
    "df_labels = pd.read_json(CLASS_LABELS_FLAT).explode('labels').fillna('none of the above')\n",
    "df_labels.rename(columns={\"data\":\"url\", \"labels\":\"class\"}, inplace=True)\n",
    "df_labels[\"asset\"] = df_labels['url'].replace(regex={r'^' + WORKSHOP_BASE + '/': ''})\n",
    "print(f\"Loaded a total of {len(df_labels)} labels across {len(df_labels['asset'].unique())} samples and {len(df_labels['class'].unique())} classes.\")\n",
    "\n",
    "# dataframe for tracking performance...\n",
    "df_performance = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and Text processing\n",
    "Our first classifiers will be constructed by using classical mapping of the class definitions through text search and mapping to tag names.  Using the classes loaded above, let's perform a quick mapping.  \n",
    "\n",
    "* **text2doc** - this function will allow us to strip out [stop words](https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/); we'll use it both for the `raw match` and the `nlp embedding` models\n",
    "* **doc2vec** - this function will take a filtered text string and produce a numerical embedding\n",
    "\n",
    "This processing will be used to test the perfomance of two text-based methods:\n",
    "\n",
    "1. direct string matching (e.g. find a tag `gift` that matches the definition of `gift`) and \n",
    "2. embedding-based matching by `k` nearest neighbors.\n",
    "\n",
    "**A-ha Questions** *(that means you probably get it)*\n",
    "* Q: Wait, where does the extra text come from? \n",
    "    * A: How insightful! we use both the class name and the defintion to compute textual features.  This is a reasonable step because those textual prompts would be provided by the stakeholder and are provided to the user labeler in subsequent notebooks.\n",
    "* Q: What is embedding?\n",
    "    * A: Embedding is a technique that assigns a numerical vector to a term based on its co-occurence with other words, which is often attributed to an early Google algorithm called [word2vec](https://en.wikipedia.org/wiki/Word2vec).  Methods have progressed since then but of course, this co-occurence is computed from statistics in a training corpus.  In the cell below, we load a spaCy model [en_core_web_md](https://spacy.io/models/en) trained on the [Common Crawl dataset](https://nlp.stanford.edu/projects/glove/) dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP model and flattened featured ready to go!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from scipy import spatial\n",
    "from pathlib import Path\n",
    "\n",
    "def text2doc(nlp, tag_raw):\n",
    "    \"\"\"Given a raw text input, tokenize it to remove stop words\"\"\"\n",
    "    return [x for x in nlp(tag_raw) if not x.is_stop and not x.is_punct]\n",
    "\n",
    "def doc2vec(nlp, doc, target_domain=None):\n",
    "    \"\"\"Given a specific model, clean line of text into an output embedding space\"\"\"\n",
    "    # https://spacy.io/usage/vectors-similarity\n",
    "    if target_domain is None:\n",
    "        target_domain = nlp.vocab\n",
    "    if type(doc) != list:\n",
    "        doc = [doc]\n",
    "    tag_doc = None\n",
    "    for token in doc:\n",
    "        tag_id = target_domain.strings[token.text]\n",
    "        if tag_id in target_domain.vectors:   # search existing one\n",
    "            new_vec = target_domain.vectors[tag_id]\n",
    "        elif type(token)==str:\n",
    "            new_vec = nlp(token).vector\n",
    "        else:\n",
    "            new_vec = token.vector\n",
    "        if tag_doc is None:\n",
    "            tag_doc = new_vec\n",
    "        else:\n",
    "            tag_doc += new_vec\n",
    "    return tag_doc\n",
    "\n",
    "# doc = text2doc(nlp, \"this is a phrase to clean\")\n",
    "# vec = doc2vec(nlp, doc)\n",
    "\n",
    "# also load our spacy NLP model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "print(\"NLP model ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers 1+2: Text Token Matching and Embedding\n",
    "Proceed to process by matching by raw keyword and NLP tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and embedding classes...\n",
      "Lookup specific tags by match...\n",
      "Count of new text-based mapping: 1804...\n",
      "Shape of embedded tag matrix: (1763, 300)...\n"
     ]
    }
   ],
   "source": [
    "# let's plow through our class dataframe and do the cleaning and mapping\n",
    "# print([x for x in df_classes[df_classes['primary']==1].iloc])\n",
    "print(\"Tokenizing and embedding classes...\")\n",
    "list_tokens = [\"\"] * len(df_classes)\n",
    "list_vect = [np.ndarray((1, nlp.vocab.vectors.shape[1]))] * len(df_classes)\n",
    "for idx in range(len(df_classes)):  # iterate row indexes\n",
    "    row = df_classes.iloc[idx]\n",
    "    # tokenize to remove tags, return tokens\n",
    "    doc = text2doc(nlp, row[\"definition\"]) + text2doc(nlp, row[\"class\"])\n",
    "    list_tokens[idx] = [str(x) for x in doc]\n",
    "    # convert to an embedding array\n",
    "    list_vect[idx] = doc2vec(nlp, doc)\n",
    "df_classes[\"token\"] = list_tokens\n",
    "df_classes[\"embedding\"] = list_vect\n",
    "\n",
    "# now collect all of the tags that match simple text bag\n",
    "print(\"Lookup specific tags by match...\")\n",
    "list_tags = df_flatten['tag'].unique()\n",
    "map_tokens = {}\n",
    "embed_tokens = np.ndarray((len(list_tags), nlp.vocab.vectors.shape[1]))\n",
    "for idx in range(len(list_tags)):   # iterate through full list of known tags\n",
    "    doc = text2doc(nlp, list_tags[idx])\n",
    "    for x in doc:  # create map/reference to each term\n",
    "        x = str(x)\n",
    "        if x not in map_tokens:  # first time to see this tag?\n",
    "            map_tokens[x] = []\n",
    "        map_tokens[x].append(idx)   # save reference to original tag set\n",
    "    embed_tokens[idx, :] = doc2vec(nlp, doc)  # compute embedding \n",
    "\n",
    "# df_classes[\"token\"] = list_tokens\n",
    "# df_classes[\"embedding\"] = list_vect\n",
    "print(f\"Count of new text-based mapping: {len(map_tokens)}...\")\n",
    "print(f\"Shape of embedded tag matrix: {embed_tokens.shape}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 1\n",
    "The scores for \"classifier 1\" are the `max()` of all scores of tags that had a string-based match against the name or definition of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e2541f08c44b678f29c8d5a8cfe15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Run:', options=('holiday_match', 'halloween_match', 'gift_giving_mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_results = {}\n",
    "for idx in range(len(df_classes)):  # iterate classes for evaluation\n",
    "    row = df_classes.iloc[idx]\n",
    "    tag_match = []\n",
    "    for x in row[\"token\"]:  # scan our tokens for this class\n",
    "        if x in map_tokens:   # now check for hits in tokenized tags\n",
    "            tag_match += map_tokens[x]   # found one? save all of the original references\n",
    "    tag_match = [list_tags[x] for x in tag_match]   # dereference to actual tag text\n",
    "    # print(tag_match)\n",
    "    df_sub = df_flatten[df_flatten['tag'].isin(tag_match)]  # grab dataframe that matched one tag\n",
    "    #print(df_sub)\n",
    "    df_score = df_sub.groupby(['asset'])['score'].max().reset_index(drop=False)\n",
    "    #print(df_score, row[\"class\"])\n",
    "\n",
    "    metrics_obj, df_scored = classifier_score(df_score, df_labels, row['class'])\n",
    "    dict_results[row['class']] = {'class':row['class'], 'method':'match', 'token':tag_match,\n",
    "                                  'metrics': metrics_obj, 'scored': df_scored, \"details\": \"\"}\n",
    "\n",
    "# save the results \n",
    "def result_update(df_performance, dict_results):\n",
    "    df_new = pd.DataFrame(dict_results.values())\n",
    "    df_new['id'] = [f\"{x['class']} {x['method']}\".replace(' ', '_') for i,x in df_new.iterrows()]\n",
    "    df_new.set_index('id', inplace=True, drop=True)\n",
    "    if df_performance is None:   # no prior records\n",
    "        df_performance = df_new\n",
    "    else:   # update or insert new performance records\n",
    "        for i,r in df_new.iterrows():\n",
    "            df_performance.loc[i] = r\n",
    "    df_performance.to_pickle(MODEL_PERFORMANCE)\n",
    "    return df_performance\n",
    "    \n",
    "# create a quick interaction grid for display\n",
    "def result_visualize(run_name):\n",
    "    global df_performance\n",
    "    if not run_name in df_performance.index:\n",
    "        print(f\"Error: Couldn't find the specified run {run_name} in results!\")\n",
    "    class_name = df_performance.loc[run_name, \"class\"]\n",
    "    display(f\"Matched Tags (for class {class_name}): {df_performance.loc[run_name, 'token']}\")\n",
    "    classifier_plot(df_performance.loc[run_name, 'metrics'], df_performance.loc[run_name, 'scored'])\n",
    "    results_display(df_performance.loc[run_name, 'scored'], 16, title=class_name)\n",
    "\n",
    "# save results\n",
    "df_performance = result_update(df_performance, dict_results)\n",
    "            \n",
    "# use widget interaction basic    \n",
    "dropdown = widgets.interactive(result_visualize, run_name=widgets.Dropdown(\n",
    "    options=list(df_performance[df_performance['method']==\"match\"].index),  # send run names\n",
    "    description='Run:',\n",
    "    disabled=False,\n",
    "))\n",
    "output = dropdown.children[-1]  # anti-flicker trick (https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html#Flickering-and-jumping-output)\n",
    "output.layout.height = '750px'  # disable this if you make your output window longer!\n",
    "display(dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 1 - Post Mortem\n",
    "\n",
    "What did we learn from this experiment?  Briefly, we confirmed that text matching alone is probablby too simplistic and can lead to far too many tokens being selected for a class.\n",
    "1. Token matching had lots of errors and went outside of the main goal for each\n",
    "2. Performance with matching never really went above random for AUC and PR was similarly erratic.\n",
    "3. There are no tuning knobs to take out spurious mappings, like the term `scene`, which was included in the definition of many classes, but was likely more of a stop word than something helpful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 2\n",
    "The scores for \"classifier 2\" are the weighted combination of the scores and nearest N neighbors in an NLP-based embedding space, as determined by a class name and definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3489c0cd140a4284bceabf73de3680c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, continuous_update=False, description='knn', max=30, min=5, step=5), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "embed_tree = BallTree(embed_tokens, leaf_size=16)\n",
    "def classify_embed(kNN=10):\n",
    "    dict_results = {}\n",
    "    for idx in range(len(df_classes)):  # iterate classes for evaluation\n",
    "        row = df_classes.iloc[idx]\n",
    "        tag_match = []\n",
    "\n",
    "        tag_dist, tag_idx = embed_tree.query(row['embedding'].reshape(1, len(row['embedding'])), k=kNN)\n",
    "        # normalize into a \"similarity\" weight instead of distance with min-max normalization\n",
    "        tag_sim = 1 - normalize(tag_dist, norm='max', axis=1)\n",
    "        \n",
    "        # dereference to actual tag text\n",
    "        tag_match = {list_tags[tag_idx[0][i]]:tag_sim[0][i] for i in range(len(tag_idx[0]))}\n",
    "        # print(tag_match.keys())\n",
    "        \n",
    "        # print(tag_match)\n",
    "        df_sub = df_flatten[df_flatten['tag'].isin(tag_match.keys())].copy()  # grab dataframe that matched one tag\n",
    "        for k in tag_match:   # scale by the weight \n",
    "            df_sub.loc[df_sub['tag']==k, 'score'] *= tag_match[k]\n",
    "        \n",
    "        #print(df_sub)\n",
    "        df_score = df_sub.groupby(['asset'])['score'].max().reset_index(drop=False)\n",
    "        #print(df_score, row[\"class\"])\n",
    "\n",
    "        metrics_obj, df_scored = classifier_score(df_score, df_labels, row['class'])\n",
    "        dict_results[row['class']] = {'class':row['class'], 'method':f'embed', \n",
    "                                      'token':list(tag_match.keys()), \"details\":str(kNN),\n",
    "                                      'metrics': metrics_obj, 'scored': df_scored}\n",
    "    return dict_results\n",
    "\n",
    "def result_remap(knn, run_name):\n",
    "    global df_performance\n",
    "    if len(df_performance[df_performance[\"details\"]==str(knn)]) == 0:\n",
    "        dict_results = classify_embed(knn)\n",
    "        df_performance = result_update(df_performance, dict_results)   # save results\n",
    "    result_visualize(run_name)\n",
    "\n",
    "            \n",
    "# use widget interaction basic    \n",
    "dropdown = widgets.interactive(result_remap, \n",
    "    knn=widgets.IntSlider(min=5, max=30, step=5, value=10, continuous_update=False),\n",
    "    run_name=widgets.Dropdown(\n",
    "    options=list(df_performance[df_performance['method']==\"embed\"].index),  # send run names\n",
    "    description='Run:',\n",
    "    disabled=False,\n",
    "))\n",
    "output = dropdown.children[-1]  # anti-flicker trick (https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html#Flickering-and-jumping-output)\n",
    "output.layout.height = '750px'  # disable this if you make your output window longer!\n",
    "display(dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 2 - Post Mortem\n",
    "\n",
    "What did we learn from this experiment?  Briefly, we saw that semantic embedding **does** help to pick better metadata tags for a given class. However, similar issues for stop words came up that may be hard to solve algorithmically.\n",
    "1. Semantic embedding got much better tags that weren't direct matches.\n",
    "2. We hit a sweet spot with `kNN=10` (or 10 nearest neighbors) from semantic embedding, which generally resulted in the hgiest AUC curves.\n",
    "3. The PR curve looked much healthier with this classifier, often above the break-even line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-level Video and Audio Features\n",
    "Our final classifier requires supervision to train.  Specifically, whereas the other two text-based classifiers used only the class name and description, we'll need explicit labels of whether or not a class is valid to continue.  Looking back at the list of [ContentAI extractors](https://www.contentai.io/docs/extractors), we'll be working with two wrapped extractors that produce [video 3DCNN](https://www.contentai.io/docs/dsai_videocnn) features and [audio VGGish](https://www.contentai.io/docs/dsai_vggish) features.  \n",
    "\n",
    "The native output of each extractor is a set of numerical features at regular time interval, according to its sample rate.  This is useful in long-form video because it helps us detect with 30s segment of a two hour comedy.  Luckily, our source data (youtube clips) have been segmented into 15s clips already, so we can actually aggregate the timed feature outputs into a single vector for each video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening Features\n",
    "This block loads the raw features and flattens them to a single row via averaging across time samples and then applying unit normalization.  The normalization step is a common practice that makes the feature more amenable to DNN and linear regressor functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting 1016 audio files in path 'packages/content/vmlr-workshop'...\n",
      "Ingesting 1016 video files in path 'packages/content/vmlr-workshop'...\n",
      "Found audio 1013 samples, 1013 video, and 1033 combined samples\n",
      "Found 1033 rows with these data columns... ['asset', 'audio', 'video', 'combined']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "def feature_flatten(x, include_norm=True):\n",
    "    if len(x) == 1:  # just one item in series\n",
    "        return x.values\n",
    "    if len(x.shape) > 1:\n",
    "        x = x.mean(axis=0)\n",
    "    vec = np.reshape(x, (1, x.shape[0]))\n",
    "    vec_norm = vec * 1 / feature_l2norm(vec)\n",
    "    return vec_norm\n",
    "\n",
    "def feature_l2norm(x):\n",
    "    # print(f\"NORM: {x.shape, type(x), len(x), x}\")\n",
    "    return np.linalg.norm(x.astype(np.float32), 2)\n",
    "\n",
    "path_features = Path(AGG_AVFEAT)\n",
    "if path_features.exists():\n",
    "    print(f\"Skipping re-create of feature file '{str(path_features)}'...\")\n",
    "    df_avfeature = pd.read_pickle(str(path_features))\n",
    "else:\n",
    "    df_avfeature = None\n",
    "    num_files = 0\n",
    "    path_content = Path(\"packages/content/vmlr-workshop\")\n",
    "\n",
    "    list_feat = []\n",
    "    list_asset = []\n",
    "    list_files = list(path_content.rglob(\"dsai_vggish/data.hdf5\"))\n",
    "    print(f\"Ingesting {len(list_files)} audio files in path '{str(path_content)}'...\")\n",
    "    for path_file in list_files:  # search for flattened files\n",
    "        with h5py.File(str(path_file),'r') as f:\n",
    "            path_asset = Path(*path_file.parent.relative_to(path_content).parts[:2])\n",
    "            for asset_key in f.keys():\n",
    "                nd_new = feature_flatten(f[asset_key][()])   # read the numpy array, flatten\n",
    "                list_asset.append(str(path_asset))   # save new feature\n",
    "                list_feat.append(nd_new)\n",
    "                #if nd_feat is None:\n",
    "                #    nd_feat = nd_new\n",
    "                #else:\n",
    "                #    print(nd_new.shape, nd_feat.shape)\n",
    "                #    nd_feat = np.append(nd_feat, nd_new, 0)  # concat feature\n",
    "    df_audio = pd.DataFrame({\"asset\": list_asset, \"audio\":list_feat}).set_index(\"asset\")\n",
    "    \n",
    "    list_feat = []\n",
    "    list_asset = []\n",
    "    list_files = list(path_content.rglob(\"dsai_videocnn/data.hdf5\"))\n",
    "    print(f\"Ingesting {len(list_files)} video files in path '{str(path_content)}'...\")\n",
    "    for path_file in list_files:  # search for flattened files\n",
    "        with h5py.File(str(path_file),'r') as f:\n",
    "            path_asset = Path(*path_file.parent.relative_to(path_content).parts[:2])\n",
    "            for asset_key in f.keys():\n",
    "                nd_new = feature_flatten(f[asset_key][()])   # read the numpy array, flatten\n",
    "                list_asset.append(str(path_asset))   # save new feature\n",
    "                list_feat.append(nd_new)\n",
    "    df_video = pd.DataFrame({\"asset\": list_asset, \"video\":list_feat}).set_index(\"asset\")\n",
    "    df_avfeature = df_audio.join(df_video, how='inner').reset_index()  # merge audio and video features\n",
    "    print(f\"Found audio {len(df_audio)} samples, {len(df_video)} video, and {len(df_avfeature)} combined samples\")\n",
    "    # uh oh, looks like there is a disparity between audio and video features!\n",
    "    #    that happens, so we'll just have to accomodate for it later...\n",
    "\n",
    "    list_combined = []\n",
    "    for idx, row in df_avfeature.iterrows():\n",
    "        nd_new = np.append(row['video'], row['audio'], axis=1)\n",
    "        nd_new *= 1 / feature_l2norm(nd_new)\n",
    "        list_combined.append(nd_new)\n",
    "    df_avfeature[\"combined\"] = list_combined\n",
    "    df_avfeature.to_pickle(str(path_features))\n",
    "\n",
    "print(f\"Found {len(df_avfeature)} rows with these data columns... {list(df_avfeature.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 3: Supervised Audio and Video \n",
    "In this section we'll train a a/v-based model, but using some labels.  The general expectation is that this model should perform the best because it is specifically trained for this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c7ee9dd5944635bae22fd0618b9395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Class Name:', options=('holiday_avfeat', 'halloween_avfeat', 'giftâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "def classify_av(feature=\"combined\", score_calibrate=False):\n",
    "    cv_folds = 5 if score_calibrate else 10\n",
    "    cv_jobs = -1  # -1 is auto, otherwise specific number\n",
    "    dict_results = {}\n",
    "    for idx in range(len(df_classes)):  # iterate classes for evaluation\n",
    "        row = df_classes.iloc[idx]\n",
    "        tag_match = []\n",
    "\n",
    "        df_label_sub = df_labels[df_labels[\"class\"]==row[\"class\"]]  # subselect for this class\n",
    "        df_feat = df_avfeature.set_index(\"asset\").copy()   # get slice of right features\n",
    "        df_feat = df_feat.join(df_label_sub.set_index(\"asset\"), how=\"left\").fillna(0)  # join with labels\n",
    "        df_feat[\"class\"] = df_feat[\"class\"].apply(lambda x: 1 if x != 0 else 0).astype(int)  # blank out text\n",
    "        \n",
    "        model = LogisticRegression()  # basic logistic regression\n",
    "        if score_calibrate:   # try to re-calibrate outputs for better threshold?\n",
    "            model = CalibratedClassifierCV(model, method=\"sigmoid\")\n",
    "        probs = cross_val_predict(model, np.vstack(df_feat[feature]), \n",
    "                                  df_feat[\"class\"], cv=cv_folds, \n",
    "                                  n_jobs=cv_jobs, method='predict_proba')\n",
    "        df_feat[\"score\"] = probs[:,1]  # grab prediction for second class\n",
    "        df_feat = df_feat.reset_index().drop(columns=['class'])  # reset index, drop label\n",
    "\n",
    "        metrics_obj, df_scored = classifier_score(df_feat[[\"asset\", \"score\"]], df_labels, row['class'])\n",
    "        dict_results[row['class']] = {'class':row['class'], 'method':f'avfeat', \n",
    "                                      'token':['audio', 'video'], \"details\":f\"{feature}_{score_calibrate}\",\n",
    "                                      'metrics': metrics_obj, 'scored': df_scored}\n",
    "    return dict_results\n",
    "\n",
    "def result_retrain(run_name, modality, calibrate):\n",
    "    global df_performance\n",
    "    details_mode = f\"{modality}_{calibrate}\"\n",
    "    if len(df_performance[df_performance[\"details\"]==details_mode]) == 0:\n",
    "        print(\"Model condition change detected, retraining classifier...\")\n",
    "        dict_results = classify_av(modality, calibrate)\n",
    "        df_performance = result_update(df_performance, dict_results)   # save results\n",
    "    result_visualize(run_name)\n",
    "\n",
    "            \n",
    "# use widget interaction basic    \n",
    "dropdown = widgets.interactive(result_retrain\n",
    "    ,run_name=widgets.Dropdown(\n",
    "        options=list(df_performance[df_performance['method']==\"avfeat\"].index),  # send run names\n",
    "        value=list(df_performance[df_performance['method']==\"avfeat\"].index)[0],\n",
    "        description='Class Name:',\n",
    "        disabled=False)\n",
    "    ,modality=widgets.Dropdown(options=['combined', 'audio', 'video'], description='Modality:')\n",
    "    ,calibrate=widgets.Checkbox(value=False, description='Score re-calibration')\n",
    ")\n",
    "output = dropdown.children[-1]  # anti-flicker trick (https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html#Flickering-and-jumping-output)\n",
    "# output.layout.height = '750px'  # disable this if you make your output window longer!\n",
    "display(dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 3 - Post Mortem\n",
    "\n",
    "What did we learn from this experiment?  We saw that feature-based analysis does a great job at finding things that look or sound a lot like what we've seen before.  Unfortunately, as in real life, the classifier hadn't seen enough of the world and its recall of diverse conditions us poor.  Additionally, this under-training effected the values of the score output, which is more erratic than other methods.\n",
    "1. A/V features are great for finding content with slight perturbations to what was labeled true.\n",
    "2. Combined audio and video features seem to always perform better than audio or video alone.\n",
    "3. Additional score calibration may be required for better threshold binarization.\n",
    "4. As always with cold-start machine learning, more data would help to grow this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Model Material\n",
    "\n",
    "Another notebook done! Now models trained on text features, embeddings, and audio-video features is under your belt.  One thing to consider now is the price of those supervised labels, which often gave us the best performance.  \n",
    "\n",
    "If you've ever proposed such an idea to a manager or business partner, some of these questions are likely to be first responses... How do you get those labels?  How many labeled examples do you need?  How long will label collection and stability take?\n",
    "\n",
    "Fret not, for in [notebook C](C_labeling.ipynb) *(that link may not work)* we will focus on programmatic use of an internal label acquisition softare, [LabelQuest](https://lq.web.DOMAIN/), which has been configured for self-service answers to some of these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
