{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning Workshop: Content Insights 2020](assets/mlci_banner.jpg)\n",
    "\n",
    "# Machine Learning Workshop: Content Insights 2020\n",
    "\n",
    "Welcome to the workshop notebooks!  These notebooks are designed to give you a walk through the steps of creating a model, refining it with user labels, and testing it on content.  You can access the main [workshop forum page](https://INFO_SITE/forums/html/forum?id=241a0b77-7aa6-4fef-9f25-5ea351825725&ps=25), the [workshop files repo](https://INFO_SITE/communities/service/html/communityview?communityUuid=fb400868-b17c-44d8-8b63-b445d26a0be4#fullpageWidgetId=W403a0d6f86de_45aa_8b67_c52cf90fca16&folder=d8138bef-9182-4bdc-8b12-3c88158a219c), or the [symposium home page](https://software.web.DOMAIN) for additional help.\n",
    "\n",
    "The notebooks are divided into five core components: (A) setup & data, (B) model exploration, (C) labeling, (D) active labeling, (E) and deployment.  You are currently viewing the *setup & data* workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for running the workshop; we'll repeat these in the top line of each workbook.\n",
    "#   why repeat them? the backup routine only serializes .ipynb files, so others will need \n",
    "#   to be downloaded again if your compute instance restarts (a small price to pay, right?)\n",
    "\n",
    "WORKSHOP_BASE = \"https://vmlr-workshop.STORAGE\"\n",
    "# WORKSHOP_BASE = \"http://content.research.DOMAIN/projects/mlci_2020\"\n",
    "AGG_METADATA = \"agg_metadata.pkl.gz\"     # custom file for merged metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Dependency Downloads\n",
    "\n",
    "This section will grab and install other package files required for the \n",
    "execution of this workshop.  This may be required if you did not start from the \n",
    "all-in-one package download.  \n",
    "\n",
    "* `packages` - contains installed packages that may not exist in other public repos\n",
    "* `data` - contains the data that will be used in this workshop in [pickled](https://docs.python.org/3/library/pickle.html) and [hdf5](https://docs.h5py.org/en/stable/) file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinone/Documents/projects/miracle/ml_hack2020/cmlp/work/packages/lq-latest-py3-none-any.whl already exists!\n",
      "/Users/quinone/Documents/projects/miracle/ml_hack2020/cmlp/work/packages/features_tag.tgz already exists!\n",
      "/Users/quinone/Documents/projects/miracle/ml_hack2020/cmlp/work/packages/features_binary.tgz already exists!\n",
      "... file download complete.\n",
      "Installing packages...\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n",
      "Expanding features...\n",
      "...all setup operations complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ATT_JUPYTER = False\n",
    "for k in os.environ:   # scan some environment vars\n",
    "    if \"user\" in k.lower():   # found user, check setting\n",
    "        if \"DOMAIN\" in os.environ[k].lower():\n",
    "            ATT_JUPYTER = True   # found AT&T, set marker\n",
    "\n",
    "proxies = None\n",
    "if ATT_JUPYTER:   # switch for proxy setting\n",
    "    # os.environ['http_proxy'] = 'http://PROXY:8080'\n",
    "    # os.environ['https_proxy'] = 'http://PROXY:8080'\n",
    "    os.environ['no_proxy'] = '*.DOMAIN'\n",
    "    proxies = {\n",
    "        \"http\": \"http://pxyapp.proxy.DOMAIN:8080\",\n",
    "        \"https\": \"http://pxyapp.proxy.DOMAIN:8080\",\n",
    "    }\n",
    "    os.environ['http_proxy'] = proxies['http']\n",
    "    os.environ['https_proxy'] = proxies['https']\n",
    "\n",
    "files = {\n",
    "    \"lq-latest-py3-none-any.whl\": f\"{WORKSHOP_BASE}/packages/lq-latest-py3-none-any.whl\"\n",
    "    , \"features_tag.tgz\": f\"{WORKSHOP_BASE}/packages/features_tag.tgz\"\n",
    "    , \"features_binary.tgz\": f\"{WORKSHOP_BASE}/packages/features_binary.tgz\"\n",
    "}\n",
    "\n",
    "def remote_download(dict_files, proxies, dir_dest=\"packages\", overwrite=False):\n",
    "    import requests\n",
    "\n",
    "    path_dest = Path(dir_dest)\n",
    "    if not path_dest.exists():\n",
    "        path_dest.mkdir(parents=True)\n",
    "\n",
    "    for name, location in files.items():\n",
    "        path_local = path_dest.joinpath(name)\n",
    "        if path_local.exists() and not overwrite:\n",
    "            print(f\"{str(path_local.resolve())} already exists!\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Getting file '{location}'\")\n",
    "        r = requests.get(location, proxies=proxies, stream=True)\n",
    "        print(f\"Writing to file {name}\")\n",
    "        with path_local.open('wb') as f:\n",
    "            for chunk in r.iter_content(4096):\n",
    "                f.write(chunk)\n",
    "\n",
    "# consider changing this to True if you have odd install errors\n",
    "remote_download(files, overwrite=False, proxies=proxies)   \n",
    "print(\"... file download complete.\")\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "\n",
    "# the labelquest client library, this is mostly used in workbook 'B'\n",
    "!pip install -q --no-cache-dir --no-index packages/lq-latest-py3-none-any.whl\n",
    "\n",
    "# some visualization helpers for the workshop\n",
    "!pip install -q --no-cache-dir --no-index ipywidgets\n",
    "\n",
    "# include basic text mapping utility and loading low level features\n",
    "!pip install -q --no-cache-dir --no-index spacy h5py\n",
    "\n",
    "# check out this URL for other text models, but since we're not using it much, a smaller version is okay \n",
    "#    https://spacy.io/models/en\n",
    "!python -m spacy download en_core_web_md -q --no-cache-dir --no-index\n",
    "\n",
    "print(\"Expanding features...\")\n",
    "!cd packages && ls *.tgz | xargs -I {} tar -zxf {}\n",
    "\n",
    "print(\"...all setup operations complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook A: Initilizing Data and Features\n",
    "\n",
    "Ready to get started *(technically you already did)*?!  In this section we'll explore timed metadata and merge it into some more easily usable [pandas DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html).  Specifically, we'll merge the raw output from many assets and content analysis tools.  Mmkay, let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Textual Features and Tags\n",
    "\n",
    "In this section, we'll take our first look at the timed metadata. Specifically, this data has been computed within the [ContentAI](https://www.contentai.io/) platform and downloaded with the steps above.  ContentAI is a flexible cloud-native platform that can accept a content reference and run one or more [extractors](https://www.contentai.io/docs/extractors) to provide metadata, processed video, etc.  \n",
    "\n",
    "In this workshop, we'll be looking at some of the tags and recognition features that come from the [Azure extrator](https://www.contentai.io/docs/azure-videoindexer-api) which wraps many of the features from the [Azure Video Indexer](https://azure.microsoft.com/en-us/services/media-services/video-indexer/) service in a secure fashion.  \n",
    "```\n",
    "content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer\n",
    "content/vmlr-workshop/gifts/vid_gift_give_take_9-2-of-14.mp4/batches/1hl1l0V3BNumdsZc3DaAJd3JlB2/azure_videoindexer\n",
    "content/vmlr-workshop/xmas/vid_xmas_8-28-of-49.mp4/batches/1hiPieI6mb2Dyzzsg84TCI5cCmM/azure_videoindexer\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-34-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-19-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer\n",
    "```\n",
    "\n",
    "Further, we'll also be looking at a normalized (or flattened) version of the data produced by the [DSAI Metadata Flattener](https://www.contentai.io/docs/dsai_metadata_flatten) ([code repo](https://CODE_SITE/projects/ST_VMLR/repos/contentai-metadata-flatten/browse)) which has been rendered to CSVs.  \n",
    "```\n",
    "content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/gifts/vid_gift_give_take_9-2-of-14.mp4/batches/1hl1l0V3BNumdsZc3DaAJd3JlB2/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/xmas/vid_xmas_8-28-of-49.mp4/batches/1hiPieI6mb2Dyzzsg84TCI5cCmM/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-34-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-19-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Insights\n",
    "As an example, let's parse and store the flattened data for Azure output, activity output, and moderation output from the flattener service.\n",
    "\n",
    "\n",
    "For inquisitive minds, the original data from the extractors is also include, typically as a simple `data.json` in their corresponding diectory.  If you've got the hang of it, try to figure out what other extractors have been run for this asset.\n",
    "\n",
    "\n",
    "```\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_videocnn/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_videocnn/data.hdf5\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_activity_classifier/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_dsai_activity_classifier.csv.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_dsai_moderation.csv.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/wbTimeTaggedMetadata.json.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_vggish/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_vggish/data.hdf5\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_moderation_image/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.csv\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.ttml\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.txt\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.vtt\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.srt\n",
    "```\n",
    "\n",
    "(answer for above...)\n",
    "* **input path** - `content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4`\n",
    "* **nested job id** - `batches/1hhadDBuEtRUPd6v8vCr5H3346r`\n",
    "* **extractor and data file** - `dsai_videocnn/data.json`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping re-create of metadata file 'agg_metadata.pkl.gz'...\n",
      "New columns in this data... ['time_begin', 'source_event', 'tag_type', 'time_end', 'time_event', 'tag', 'score', 'details', 'extractor', 'asset']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path_metadata = Path(AGG_METADATA)\n",
    "if path_metadata.exists():\n",
    "    print(f\"Skipping re-create of metadata file '{str(path_metadata)}'...\")\n",
    "    df_flatten = pd.read_pickle(str(path_metadata))\n",
    "else:\n",
    "    df_flatten = None\n",
    "    num_files = 0\n",
    "    path_content = Path(\"packages/content/vmlr-workshop\")\n",
    "    list_files = list(path_content.rglob(\"csv_flatten*.csv*\"))\n",
    "    print(f\"Ingesting {len(list_files)} flatten files in path '{str(path_content)}'...\")\n",
    "    for path_file in list_files:  # search for flattened files\n",
    "        df_new = pd.read_csv(path_file)\n",
    "        # FROM content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz -> \n",
    "        # TO halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten (relative_to)\n",
    "        # TO halloween/vid_halloween_0-13-of-23.mp4  (joining base path parts)\n",
    "        path_asset = Path(*path_file.parent.relative_to(path_content).parts[:2])\n",
    "        df_new['tag'] = df_new['tag'].str.lower()   # lower case the tags\n",
    "        df_new['details'] = df_new['details'].fillna('').str.lower()   # lower case the enhanced information\n",
    "        df_new['asset'] = str(path_asset)\n",
    "        if df_flatten is None:   # first one we saw\n",
    "            df_flatten = df_new\n",
    "        else:\n",
    "            df_flatten = df_flatten.append(df_new, ignore_index=True)   # append new dataframe\n",
    "        num_files += 1\n",
    "        if num_files % 500 == 0:\n",
    "            print(f\"... read {num_files}...\")\n",
    "    df_flatten.reset_index(drop=True, inplace=True)  # drop prior index\n",
    "    df_flatten.to_pickle(str(path_metadata))\n",
    "    print(f\"Wrote {num_files} aggregations to file '{str(path_metadata)}'...\")\n",
    "\n",
    "print(f\"New columns in this data... {list(df_flatten.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting tag statistics\n",
    "Let's plot some statistics about tags, both their numbers and their names.  First, a histogram of how many unique and total tags were present for an asset.  This plot helps us find average number of tags, both in raw counts and unique tags for an asset.  Second, an average and raw count of the top `N` tags found from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44300fe88b55411e90974810aff47343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatRangeSlider(value=(0.5, 1.0), continuous_update=False, description='Score Range:', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this is a handy update function\n",
    "def tag_count_hist(x):\n",
    "    x = (round(x[0], 2), round(x[1], 2))\n",
    "    df_sub = df_flatten[(df_flatten['score'] >= x[0]) & (df_flatten['score'] <= x[1])]\n",
    "    df_pairs = df_sub.groupby(['asset','tag']).count()['score'].reset_index()   # group by two params, reset into dataframe\n",
    "    df_unitags = df_pairs.groupby(['asset'])['score'].agg(['count','sum']).reset_index()   # group by asset to find unique tag count per asset\n",
    "    df_unitags.rename(columns={\"count\":\"unique tags\", \"sum\":\"total tags\"}, inplace=True)\n",
    "    # print(df_unitags)\n",
    "    ax = df_unitags.plot.hist(by='asset', bins=40, figsize=(12,4), alpha=0.75)\n",
    "    pl.title(f\"Histogram of Tags Counts Among Assets ({x[0]} >= Score >= {x[1]})\")\n",
    "    pl.ylabel('number of assets')\n",
    "    pl.xlabel('count of tags')\n",
    "    pl.grid()\n",
    "    pl.show()\n",
    "    \n",
    "    df_pairs = df_sub.groupby(['tag','asset']).count()['score'].reset_index()   # group by two params, reset into dataframe\n",
    "    df_unitags = df_pairs.groupby(['tag'])['score'].agg(['count','sum']).reset_index()   # group by asset to find unique tag count per asset\n",
    "    df_unitags.sort_values('sum', ignore_index=True, inplace=True, ascending=False)\n",
    "    df_unitags.rename(columns={\"count\":\"Asset Frequency\", \"sum\":\"Total Frequency\"}, inplace=True)\n",
    "    top_n = 20\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "    \n",
    "    df_topn = df_unitags.iloc[:top_n]\n",
    "    df_topn.plot.barh(ax=ax1, x='tag', width=0.8, log=True)\n",
    "    ax1.set_title(f\"Top {top_n} Tags ({x[0]} >= Score >= {x[1]})\")\n",
    "    ax1.set_ylabel('tag text')\n",
    "    ax1.set_xlabel('count of tags')\n",
    "    ax1.legend(loc=\"lower left\")\n",
    "    ax1.grid()\n",
    "    \n",
    "    skip_percent = 0.05\n",
    "    top_percent = int(len(df_unitags)*skip_percent)\n",
    "    df_topn = df_unitags.iloc[top_percent:top_percent+top_n]\n",
    "    df_topn.plot.barh(ax=ax2, x='tag', width=0.8, log=True)\n",
    "    ax2.set_title(f\"Top {top_n} (skip {skip_percent*100:1}%) Tags ({x[0]} >= Score >= {x[1]})\")\n",
    "    ax2.set_ylabel('tag text')\n",
    "    ax2.set_xlabel('count of tags')\n",
    "    ax2.legend(loc=\"lower left\")\n",
    "    ax2.grid()\n",
    "    \n",
    "    \n",
    "\n",
    "# get an interactive widget/graph\n",
    "widgets.interactive(tag_count_hist, x=widgets.FloatRangeSlider(\n",
    "    value=[0.5, 1.0],\n",
    "    step=0.05,\n",
    "    min=df_flatten['score'].min(),\n",
    "    max=df_flatten['score'].max(),\n",
    "    description='Score Range:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Intro Data Material\n",
    "\n",
    "Nice work, you've just created a useable, aggregated form of content metadata.  Consider switching over to [notebook B](B_models.ipynb) *(that link may not work)* to continue exploration and building of models using existing metadata tags."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
