{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning Workshop: Content Insights 2020](assets/mlci_banner.jpg)\n",
    "\n",
    "# Machine Learning Workshop: Content Insights 2020\n",
    "\n",
    "Welcome to the workshop notebooks!  These notebooks are designed to give you a walk through the steps of creating a model, refining it with user labels, and testing it on content.  You can access the main [workshop forum page](https://INFO_SITE/forums/html/forum?id=241a0b77-7aa6-4fef-9f25-5ea351825725&ps=25), the [workshop files repo](https://INFO_SITE/communities/service/html/communityview?communityUuid=fb400868-b17c-44d8-8b63-b445d26a0be4#fullpageWidgetId=W403a0d6f86de_45aa_8b67_c52cf90fca16&folder=d8138bef-9182-4bdc-8b12-3c88158a219c), or the [symposium home page](https://software.web.DOMAIN) for additional help.\n",
    "\n",
    "The notebooks are divided into five core components: (A) setup & data, (B) model exploration, (C) labeling, (D) active labeling, (E) and deployment.  You are currently viewing the *setup & data* workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for running the workshop; we'll repeat these in the top line of each workbook.\n",
    "#   why repeat them? the backup routine only serializes .ipynb files, so others will need \n",
    "#   to be downloaded again if your compute instance restarts (a small price to pay, right?)\n",
    "\n",
    "WORKSHOP_BASE = \"https://vmlr-workshop.STORAGE\"\n",
    "# WORKSHOP_BASE = \"http://content.research.DOMAIN/projects/mlci_2020\"\n",
    "AGG_METADATA = \"agg_metadata.pkl.gz\"     # custom file for merged metadata\n",
    "CLASS_DEFINITIONS = \"assets/classes.json\"     # provided file for class info\n",
    "CLASS_LABELS_FLAT = \"assets/labels_final.json\"     # provided file for label info\n",
    "AGG_TAG_EMBEDDING = \"agg_tag_vocab.w2v\"  # custom file for tag-based vocabulary\n",
    "AGG_LABELS = \"agg_labels.pkl.gz\"         # custom file for merged labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Dependency Downloads\n",
    "\n",
    "This section will grab and install other package files required for the \n",
    "execution of this workshop.  This may be required if you did not start from the \n",
    "all-in-one package download.  \n",
    "\n",
    "* `packages` - contains installed packages that may not exist in other public repos\n",
    "* `data` - contains the data that will be used in this workshop in [pickled](https://docs.python.org/3/library/pickle.html) and [hdf5](https://docs.h5py.org/en/stable/) file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinone/Documents/projects/miracle/ml_hack2020/cmlp/work/packages/lq-latest-py3-none-any.whl already exists!\n",
      "/Users/quinone/Documents/projects/miracle/ml_hack2020/cmlp/work/packages/features_tag.tgz already exists!\n",
      "/Users/quinone/Documents/projects/miracle/ml_hack2020/cmlp/work/packages/features_binary.tgz already exists!\n",
      "... file download complete.\n",
      "Installing packages...\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n",
      "Expanding features...\n",
      "...all setup operations complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ATT_JUPYTER = False\n",
    "for k in os.environ:   # scan some environment vars\n",
    "    if \"user\" in k.lower():   # found user, check setting\n",
    "        if \"DOMAIN\" in os.environ[k].lower():\n",
    "            ATT_JUPYTER = True   # found AT&T, set marker\n",
    "\n",
    "proxies = None\n",
    "if ATT_JUPYTER:   # switch for proxy setting\n",
    "    # os.environ['http_proxy'] = 'http://PROXY:8080'\n",
    "    # os.environ['https_proxy'] = 'http://PROXY:8080'\n",
    "    os.environ['no_proxy'] = '*.DOMAIN'\n",
    "    proxies = {\n",
    "        \"http\": \"http://pxyapp.proxy.DOMAIN:8080\",\n",
    "        \"https\": \"http://pxyapp.proxy.DOMAIN:8080\",\n",
    "    }\n",
    "    os.environ['http_proxy'] = proxies['http']\n",
    "    os.environ['https_proxy'] = proxies['https']\n",
    "\n",
    "files = {\n",
    "    \"lq-latest-py3-none-any.whl\": f\"{WORKSHOP_BASE}/packages/lq-latest-py3-none-any.whl\"\n",
    "    , \"features_tag.tgz\": f\"{WORKSHOP_BASE}/packages/features_tag.tgz\"\n",
    "    , \"features_binary.tgz\": f\"{WORKSHOP_BASE}/packages/features_binary.tgz\"\n",
    "}\n",
    "\n",
    "def remote_download(dict_files, proxies, dir_dest=\"packages\", overwrite=False):\n",
    "    import requests\n",
    "\n",
    "    path_dest = Path(dir_dest)\n",
    "    if not path_dest.exists():\n",
    "        path_dest.mkdir(parents=True)\n",
    "\n",
    "    for name, location in files.items():\n",
    "        path_local = path_dest.joinpath(name)\n",
    "        if path_local.exists() and not overwrite:\n",
    "            print(f\"{str(path_local.resolve())} already exists!\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Getting file '{location}'\")\n",
    "        r = requests.get(location, proxies=proxies, stream=True)\n",
    "        print(f\"Writing to file {name}\")\n",
    "        with path_local.open('wb') as f:\n",
    "            for chunk in r.iter_content(4096):\n",
    "                f.write(chunk)\n",
    "\n",
    "# consider changing this to True if you have odd install errors\n",
    "remote_download(files, overwrite=False, proxies=proxies)   \n",
    "print(\"... file download complete.\")\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "\n",
    "# the labelquest client library, this is mostly used in workbook 'B'\n",
    "!pip install -q --no-cache-dir --no-index packages/lq-latest-py3-none-any.whl\n",
    "\n",
    "# some visualization helpers for the workshop\n",
    "!pip install -q --no-cache-dir --no-index ipywidgets\n",
    "\n",
    "# include basic text mapping utility\n",
    "!pip install -q --no-cache-dir --no-index spacy\n",
    "\n",
    "# check out this URL for other text models, but since we're not using it much, a smaller version is okay \n",
    "#    https://spacy.io/models/en\n",
    "!python -m spacy download en_core_web_md -q --no-cache-dir --no-index\n",
    "\n",
    "print(\"Expanding features...\")\n",
    "!cd packages && ls *.tgz | xargs -I {} tar -zxf {}\n",
    "\n",
    "print(\"...all setup operations complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Textual Features and Tags\n",
    "\n",
    "In this section, we'll take our first look at the timed metadata. Specifically, this data has been computed within the [ContentAI](https://www.contentai.io/) platform and downloaded with the steps above.  ContentAI is a flexible cloud-native platform that can accept a content reference and run one or more [extractors](https://www.contentai.io/docs/extractors) to provide metadata, processed video, etc.  \n",
    "\n",
    "In this workshop, we'll be looking at some of the tags and recognition features that come from the [Azure extrator](https://www.contentai.io/docs/azure-videoindexer-api) which wraps many of the features from the [Azure Video Indexer](https://azure.microsoft.com/en-us/services/media-services/video-indexer/) service in a secure fashion.  \n",
    "```\n",
    "content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer\n",
    "content/vmlr-workshop/gifts/vid_gift_give_take_9-2-of-14.mp4/batches/1hl1l0V3BNumdsZc3DaAJd3JlB2/azure_videoindexer\n",
    "content/vmlr-workshop/xmas/vid_xmas_8-28-of-49.mp4/batches/1hiPieI6mb2Dyzzsg84TCI5cCmM/azure_videoindexer\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-34-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-19-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer\n",
    "```\n",
    "\n",
    "Further, we'll also be looking at a normalized (or flattened) version of the data produced by the [DSAI Metadata Flattener](https://www.contentai.io/docs/dsai_metadata_flatten) ([code repo](https://CODE_SITE/projects/ST_VMLR/repos/contentai-metadata-flatten/browse)) which has been rendered to CSVs.  \n",
    "```\n",
    "content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/gifts/vid_gift_give_take_9-2-of-14.mp4/batches/1hl1l0V3BNumdsZc3DaAJd3JlB2/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/xmas/vid_xmas_8-28-of-49.mp4/batches/1hiPieI6mb2Dyzzsg84TCI5cCmM/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-34-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-19-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Insights\n",
    "As an example, let's parse and store the flattened data for Azure output, activity output, and moderation output from the flattener service.\n",
    "\n",
    "\n",
    "For inquisitive minds, the original data from the extractors is also include, typically as a simple `data.json` in their corresponding diectory.  If you've got the hang of it, try to figure out what other extractors have been run for this asset.\n",
    "\n",
    "\n",
    "```\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_videocnn/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_videocnn/data.hdf5\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_activity_classifier/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_dsai_activity_classifier.csv.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_dsai_moderation.csv.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/wbTimeTaggedMetadata.json.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_vggish/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_vggish/data.hdf5\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_moderation_image/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.csv\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.ttml\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.txt\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.vtt\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.srt\n",
    "```\n",
    "\n",
    "(answer for above...)\n",
    "* **input path** - `content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4`\n",
    "* **nested job id** - `batches/1hhadDBuEtRUPd6v8vCr5H3346r`\n",
    "* **extractor and data file** - `dsai_videocnn/data.json`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping re-create of metadata file 'agg_metadata.pkl.gz'...\n",
      "New columns in this data... ['time_begin', 'source_event', 'tag_type', 'time_end', 'time_event', 'tag', 'score', 'details', 'extractor', 'asset']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path_metadata = Path(AGG_METADATA)\n",
    "if path_metadata.exists():\n",
    "    print(f\"Skipping re-create of metadata file '{str(path_metadata)}'...\")\n",
    "    df_flatten = pd.read_pickle(str(path_metadata))\n",
    "else:\n",
    "    df_flatten = None\n",
    "    num_files = 0\n",
    "    path_content = Path(\"packages/content/vmlr-workshop\")\n",
    "    list_files = list(path_content.rglob(\"csv_flatten*.csv*\"))\n",
    "    print(f\"Ingesting {len(list_files)} flatten files in path '{str(path_content)}'...\")\n",
    "    for path_file in list_files:  # search for flattened files\n",
    "        df_new = pd.read_csv(path_file)\n",
    "        # FROM content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz -> \n",
    "        # TO halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten (relative_to)\n",
    "        # TO halloween/vid_halloween_0-13-of-23.mp4  (joining base path parts)\n",
    "        path_asset = Path(*path_file.parent.relative_to(path_content).parts[:2])\n",
    "        df_new['tag'] = df_new['tag'].str.lower()   # lower case the tags\n",
    "        df_new['details'] = df_new['details'].fillna('').str.lower()   # lower case the enhanced information\n",
    "        df_new['asset'] = str(path_asset)\n",
    "        if df_flatten is None:   # first one we saw\n",
    "            df_flatten = df_new\n",
    "        else:\n",
    "            df_flatten = df_flatten.append(df_new, ignore_index=True)   # append new dataframe\n",
    "        num_files += 1\n",
    "        if num_files % 500 == 0:\n",
    "            print(f\"... read {num_files}...\")\n",
    "    df_flatten.reset_index(drop=True, inplace=True)  # drop prior index\n",
    "    df_flatten.to_pickle(str(path_metadata))\n",
    "    print(f\"Wrote {num_files} aggregations to file '{str(path_metadata)}'...\")\n",
    "\n",
    "print(f\"New columns in this data... {list(df_flatten.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting tags\n",
    "Let's plot some statistics about tags, both their numbers and their names.  First, a histogram of how many unique and total tags were present for an asset.  This plot helps us find average number of tags, both in raw counts and unique tags for an asset.  Second, an average and raw count of the top `N` tags found from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820389921032491bbe6664ae967959d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatRangeSlider(value=(0.5, 1.0), continuous_update=False, description='Score Range:', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.tag_count_hist(x)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# this is a handy update function\n",
    "def tag_count_hist(x):\n",
    "    x = (round(x[0], 2), round(x[1], 2))\n",
    "    df_sub = df_flatten[(df_flatten['score'] >= x[0]) & (df_flatten['score'] <= x[1])]\n",
    "    df_pairs = df_sub.groupby(['asset','tag']).count()['score'].reset_index()   # group by two params, reset into dataframe\n",
    "    df_unitags = df_pairs.groupby(['asset'])['score'].agg(['count','sum']).reset_index()   # group by asset to find unique tag count per asset\n",
    "    df_unitags.rename(columns={\"count\":\"unique tags\", \"sum\":\"total tags\"}, inplace=True)\n",
    "    # print(df_unitags)\n",
    "    ax = df_unitags.plot.hist(by='asset', bins=40, figsize=(10,4), alpha=0.75)\n",
    "    pl.title(f\"Histogram of Unique Tags Among Assets ({x[0]} >= Score >= {x[1]})\")\n",
    "    pl.ylabel('number of assets')\n",
    "    pl.xlabel('count of tags')\n",
    "    pl.grid()\n",
    "    pl.show()\n",
    "    \n",
    "    df_pairs = df_sub.groupby(['tag','asset']).count()['score'].reset_index()   # group by two params, reset into dataframe\n",
    "    df_unitags = df_pairs.groupby(['tag'])['score'].agg(['count','sum']).reset_index()   # group by asset to find unique tag count per asset\n",
    "    df_unitags.sort_values('sum', ignore_index=True, inplace=True, ascending=False)\n",
    "    df_unitags.rename(columns={\"count\":\"Asset Frequency\", \"sum\":\"Total Frequency\"}, inplace=True)\n",
    "    top_n = 20\n",
    "    df_topn = df_unitags.iloc[:top_n]\n",
    "    ax = df_topn.plot.barh(x='tag', figsize=(10,4), width=0.8, log=True)\n",
    "    pl.title(f\"Total and Asset Frequency of {top_n} Most Frequent Tags ({x[0]} >= Score >= {x[1]})\")\n",
    "    pl.ylabel('tag text')\n",
    "    pl.xlabel('count of tags')\n",
    "    pl.grid()\n",
    "    pl.show()\n",
    "    \n",
    "    \n",
    "\n",
    "# get an interactive widget/graph\n",
    "widgets.interact(tag_count_hist, x=widgets.FloatRangeSlider(\n",
    "    value=[0.5, 1.0],\n",
    "    step=0.05,\n",
    "    min=df_flatten['score'].min(),\n",
    "    max=df_flatten['score'].max(),\n",
    "    description='Score Range:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Ads\n",
    "\n",
    "One goal of combining timed metadata and content is for better alignment of content from WarnerMedia and ads from Xandr creatives.  The iamge below demonstrates one example where a detected keyword or scene can trigger an ad that is realted.  Without this technology, this ad spot (or inventory) may be undersold and filled with an unrelated or standard campaign ad.\n",
    "\n",
    "![Contextual Ad Product](assets/mlci_contextual.jpg)\n",
    "\n",
    "## Class Exploration\n",
    "Let's quickly load and display the target classes used in this experiment. The table below indicates the class and the definition utilized for our classifier.  The field `primary` indicates whether or not a class will be used for performance evaluations.  Some non-primary classes were also included for additional experimentation, but they will not be the focus here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>definition</th>\n",
       "      <th>primary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>holiday</td>\n",
       "      <td>holiday scenes or objects like decorated trees, presents, or character, holiday party</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>halloween</td>\n",
       "      <td>halloween scenes where one or more characters are in costume, ideally one or more characters are trick-or-treating</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gift giving</td>\n",
       "      <td>scenes of gift giving, receiving, or opening/unwrapping</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>family moments</td>\n",
       "      <td>at least two people on screen, typically familes at parties, enjoying a meal, lounging at home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shopping scenes</td>\n",
       "      <td>one or more primary actors in a store-like environment; not necessary to see their face</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class  \\\n",
       "0          holiday   \n",
       "1        halloween   \n",
       "2      gift giving   \n",
       "3   family moments   \n",
       "4  shopping scenes   \n",
       "\n",
       "                                                                                                           definition  \\\n",
       "0                               holiday scenes or objects like decorated trees, presents, or character, holiday party   \n",
       "1  halloween scenes where one or more characters are in costume, ideally one or more characters are trick-or-treating   \n",
       "2                                                             scenes of gift giving, receiving, or opening/unwrapping   \n",
       "3                      at least two people on screen, typically familes at parties, enjoying a meal, lounging at home   \n",
       "4                             one or more primary actors in a store-like environment; not necessary to see their face   \n",
       "\n",
       "   primary  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open(CLASS_DEFINITIONS, 'r') as f:\n",
    "    obj_classes = json.load(f)\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "df_classes = pd.read_json(CLASS_DEFINITIONS)\n",
    "df_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Browsing\n",
    "Below, we've created a simple method to view keyframe results from the video assets used in this workshop.  It uses simple widgets (as we did above) and displays them in a columnar form in ths notebook.  In addition to numerical performance (accuracy, AUC, etc.), we can visually inspect the performance of a classifier model with this utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180567c8249b4df1948a2e550af6ac09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Results demo</h3>'), GridBox(children=(HTML(value=\"\\n        <a href='https://v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "def display_hit_html(path_result, score, label=1, base=WORKSHOP_BASE):\n",
    "    '''\n",
    "     This function essentially convert the image url to \n",
    "     '<img src=\"'+ path + '\"/>' format. And one can put any\n",
    "     formatting adjustments to control the height, aspect ratio, size etc.\n",
    "     within as in the below example. \n",
    "    '''\n",
    "    path_parts = path_result.split('/')\n",
    "    url_full = f\"{base}/{path_parts[0]}/keyframe/{path_parts[1]}.jpg\"\n",
    "    text_color = f\" style='color:{'black' if label==1 else 'red'}' \"\n",
    "    return f\"\"\"\n",
    "        <a href='{url_full}' title='{path_result}' target='_new'><img width='100%' src='{url_full}' /></a>\n",
    "        <small {text_color}><strong>{score:0.4}</strong></small>\n",
    "        \"\"\"\n",
    "\n",
    "def results_display(df, max_results=32, num_cols=8, base=WORKSHOP_BASE, title=\"Example Results\"):\n",
    "    \"\"\"Input DataFrame with 'asset' and 'score' and show those results in columnar format.\n",
    "    Providing an additional column 'label' [as 0 or 1] will allow incorrect items to be shown in red.\"\"\"\n",
    "    width_col = f\"{round(100/num_cols)}%\"\n",
    "    #list_results.sort(reverse=True, key=lambda x: x[-1]) \n",
    "    #list_html = [widgets.HTML(path_to_image_html(*x, base=base)) for x in list_results]\n",
    "    df = df.sort_values(\"score\", ascending=False).head(max_results)\n",
    "    if \"label\" not in df.columns:\n",
    "        df[\"label\"] = 1\n",
    "    list_html = [widgets.HTML(display_hit_html(x['asset'], x['score'], x['label'], base=base)) for i,x in df.iterrows()]\n",
    "    display(widgets.VBox([widgets.HTML(f\"<h3>{title}</h3>\"), widgets.GridBox(list_html,\n",
    "               layout=widgets.Layout(grid_template_columns=f\"repeat(8, {width_col})\"))]))\n",
    "\n",
    "# demo of input as a single result, but with random scores\n",
    "df_demo = pd.DataFrame({\"asset\":[\"xmas/vid_xmas_9-62-of-67.mp4\"]*16, \n",
    "                        \"score\":np.random.rand(1, 16)[0],\n",
    "                         \"label\":np.random.randint(2, size=16)})\n",
    "results_display(df_demo, title=\"Results demo\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Result Scoring\n",
    "Similar to the code above, this function can be used for evaluation of models by a few different metrics -- but here it is aimed at objective, numerical methods.  For convenience, this method can also plot the results of both objective and subjective scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a total of 826 labels across 520 samples and 7 labels.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def classifier_score(df_prediction, df_labels, class_name):\n",
    "    \"\"\"Functiont to provide metric outputs for the evaluation of a prediction dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        df_prediction (DataFrame): dataframe containing 'asset' and 'score' as columns\n",
    "        df_labels (DataFrame): dataframe containing 'asset' and 'label' for labels\n",
    "        class_name (str): class name for evaluation against labels\n",
    "\n",
    "    Returns:\n",
    "        dict of metrics (AUC, AP, precision, recall) ({\"ap\":X, \"class\":Y, ...}) and joined dataframe\n",
    "    \"\"\"\n",
    "    metrics_obj = {\"class\":class_name}\n",
    "    \n",
    "    # clean up input labels, prune to relevant class\n",
    "    df_labels = df_labels[df_labels[\"label\"] == class_name].drop(columns=[\"etag\", \"url\"]) \n",
    "    # join labels and scores by asset, nomalize score to float\n",
    "    df_join = df_prediction.set_index('asset').join(df_labels.set_index('asset'), how=\"left\").fillna(0)  # joint at asset level, 0 for nonscoring\n",
    "    df_join[\"label\"] = df_join[\"label\"].apply(lambda x: 1 if x != 0 else 0).astype(int)\n",
    "    df_join = df_join.reset_index().sort_values(\"score\", ascending=False)\n",
    "\n",
    "    print(f\"{class_name}: Found {len(df_join)} samples from {len(df_labels)} labels and {len(df_prediction)} scores.\")\n",
    "\n",
    "    def thresh(x):\n",
    "        return 1 if x >= 0.5 else 0\n",
    "    \n",
    "    metrics_obj[\"AP\"] = metrics.average_precision_score(df_join['label'], df_join['score'])\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(df_join['label'], df_join['score'])\n",
    "    metrics_obj[\"AUC\"] = metrics.auc(fpr, tpr)\n",
    "    metrics_obj[\"Accuracy\"] = metrics.accuracy_score(df_join['label'], df_join['score'].apply(thresh))\n",
    "    metrics_obj[\"Recall\"] = metrics.recall_score(df_join['label'], df_join['score'].apply(thresh))\n",
    "    metrics_obj[\"F1\"] = metrics.f1_score(df_join['label'], df_join['score'].apply(thresh))\n",
    "    print(f\"{class_name}: {metrics_obj}\")\n",
    "        \n",
    "    # return our computation!\n",
    "    return metrics_obj, df_join\n",
    "\n",
    "def classifier_plot(metrics_obj, df_scored):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(df_scored['label'], df_scored['score'])\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "    lw = 2\n",
    "    ax1.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label=f\"AUC curve (area = {metrics_obj['AUC']:0.2})\")\n",
    "    ax1.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(df_scored['label'], df_scored['score'])\n",
    "    ax2.plot(recall, precision, color='red',\n",
    "             lw=lw, label=f\"PR Curve (F1 = {metrics_obj['F1']:0.2})\")\n",
    "    ax2.plot([1, 0], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "    \n",
    "# read label data for use later!\n",
    "df_labels = pd.read_json(CLASS_LABELS_FLAT).explode('labels').rename(columns={\"data\":\"url\", \"labels\":\"label\"})\n",
    "df_labels[\"asset\"] = df_labels['url'].replace(regex={r'^' + WORKSHOP_BASE + '/': ''})\n",
    "print(f\"Loaded a total of {len(df_labels)} labels across {len(df_labels['asset'].unique())} samples and {len(df_labels['label'].unique())} labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers 1, 2: Classifier By Text\n",
    "Our first classifier model will be constructed by using classical mapping of the class definitions through text search and mapping to tag names.  Using the classes loaded above, let's perform a quick mapping.  \n",
    "\n",
    "* **text2doc** - this function will allow us to strip out [stop words](https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/); we'll use it both for the `raw match` and the `nlp embedding` models\n",
    "* **doc2vec** - this function will take a filtered text string and produce a numerical embedding\n",
    "\n",
    "This processing will be used to test the perfomance of (1) direct string matching (e.g. find a tag `gift` that matches the definition of `gift`) and (2) embedding-based matching by `k` nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and embedding classes...\n",
      "Lookup specific tags by match...\n",
      "Count of new text-based mapping: 1804...\n",
      "Shape of embedded tag matrix: (1763, 300)...\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from scipy import spatial\n",
    "\n",
    "def text2doc(nlp, tag_raw):\n",
    "    \"\"\"Given a raw text input, tokenize it to remove stop words\"\"\"\n",
    "    return [x for x in nlp(tag_raw) if not x.is_stop and not x.is_punct]\n",
    "\n",
    "def doc2vec(nlp, doc, target_domain=None):\n",
    "    \"\"\"Given a specific model, clean line of text into an output embedding space\"\"\"\n",
    "    # https://spacy.io/usage/vectors-similarity\n",
    "    if target_domain is None:\n",
    "        target_domain = nlp.vocab\n",
    "    if type(doc) != list:\n",
    "        doc = [doc]\n",
    "    tag_doc = None\n",
    "    for token in doc:\n",
    "        tag_id = target_domain.strings[token.text]\n",
    "        if tag_id in target_domain.vectors:   # search existing one\n",
    "            new_vec = target_domain.vectors[tag_id]\n",
    "        elif type(token)==str:\n",
    "            new_vec = nlp(token).vector\n",
    "        else:\n",
    "            new_vec = token.vector\n",
    "        if tag_doc is None:\n",
    "            tag_doc = new_vec\n",
    "        else:\n",
    "            tag_doc += new_vec\n",
    "    return tag_doc\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# doc = text2doc(nlp, \"this is a phrase to clean\")\n",
    "# vec = doc2vec(nlp, doc)\n",
    "\n",
    "# let's plow through our class dataframe and do the cleaning and mapping\n",
    "# print([x for x in df_classes[df_classes['primary']==1].iloc])\n",
    "print(\"Tokenizing and embedding classes...\")\n",
    "list_tokens = [\"\"] * len(df_classes)\n",
    "list_vect = [np.ndarray((1, nlp.vocab.vectors.shape[1]))] * len(df_classes)\n",
    "for idx in range(len(df_classes)):  # iterate row indexes\n",
    "    row = df_classes.iloc[idx]\n",
    "    # tokenize to remove tags, return tokens\n",
    "    doc = text2doc(nlp, row[\"definition\"]) + text2doc(nlp, row[\"class\"])\n",
    "    list_tokens[idx] = [str(x) for x in doc]\n",
    "    # convert to an embedding array\n",
    "    list_vect[idx] = doc2vec(nlp, doc)\n",
    "df_classes[\"token\"] = list_tokens\n",
    "df_classes[\"embedding\"] = list_vect\n",
    "\n",
    "# now collect all of the tags that match simple text bag\n",
    "print(\"Lookup specific tags by match...\")\n",
    "list_tags = df_flatten['tag'].unique()\n",
    "map_tokens = {}\n",
    "embed_tokens = np.ndarray((len(list_tags), nlp.vocab.vectors.shape[1]))\n",
    "for idx in range(len(list_tags)):   # iterate through full list of known tags\n",
    "    doc = text2doc(nlp, list_tags[idx])\n",
    "    for x in doc:  # create map/reference to each term\n",
    "        x = str(x)\n",
    "        if x not in map_tokens:  # first time to see this tag?\n",
    "            map_tokens[x] = []\n",
    "        map_tokens[x].append(idx)   # save reference to original tag set\n",
    "    embed_tokens[idx, :] = doc2vec(nlp, doc)  # compute embedding \n",
    "\n",
    "# df_classes[\"token\"] = list_tokens\n",
    "# df_classes[\"embedding\"] = list_vect\n",
    "print(f\"Count of new text-based mapping: {len(map_tokens)}...\")\n",
    "print(f\"Shape of embedded tag matrix: {embed_tokens.shape}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 1\n",
    "The scores for \"classifier 1\" are the `max()` of all scores for the selected tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holiday: Found 121 samples from 182 labels and 121 scores.\n",
      "holiday: {'class': 'holiday', 'AP': 0.431933284866166, 'AUC': 0.5834876543209877, 'Accuracy': 0.33884297520661155, 'Recall': 0.975, 'F1': 0.49367088607594933}\n",
      "halloween: Found 47 samples from 150 labels and 47 scores.\n",
      "halloween: {'class': 'halloween', 'AP': 0.4147021310041902, 'AUC': 0.5478927203065134, 'Accuracy': 0.3829787234042553, 'Recall': 0.8888888888888888, 'F1': 0.5245901639344261}\n",
      "gift giving: Found 25 samples from 115 labels and 25 scores.\n",
      "gift giving: {'class': 'gift giving', 'AP': 0.3350140056022409, 'AUC': 0.6403508771929824, 'Accuracy': 0.32, 'Recall': 1.0, 'F1': 0.41379310344827586}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244a791607a04a3899c138b281c0ad7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Class:', options=('holiday', 'halloween', 'gift giving'), value='h…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_results = {}\n",
    "for idx in range(len(df_classes)):  # iterate classes for evaluation\n",
    "    row = df_classes.iloc[idx]\n",
    "    if row[\"primary\"] != 1:   # skip non-priority items\n",
    "        continue\n",
    "    tag_match = []\n",
    "    for x in row[\"token\"]:  # scan our tokens for this class\n",
    "        if x in map_tokens:   # now check for hits in tokenized tags\n",
    "            tag_match += map_tokens[x]   # found one? save all of the original references\n",
    "    tag_match = [list_tags[x] for x in tag_match]   # dereference to actual tag text\n",
    "    # print(tag_match)\n",
    "    df_sub = df_flatten[df_flatten['tag'].isin(tag_match)]  # grab dataframe that matched one tag\n",
    "    #print(df_sub)\n",
    "    df_score = df_sub.groupby(['asset'])['score'].max().reset_index(drop=False)\n",
    "    #print(df_score, row[\"class\"])\n",
    "\n",
    "    metrics_obj, df_scored = classifier_score(df_score, df_labels, row['class'])\n",
    "    dict_results[row['class']] = {'metrics': metrics_obj, 'scored': df_scored}\n",
    "\n",
    "# create a quick interaction grid for display\n",
    "def result_visualize(class_name):\n",
    "    classifier_plot(dict_results[class_name]['metrics'], dict_results[class_name]['scored'])\n",
    "    results_display(dict_results[class_name]['scored'], 16, title=class_name)\n",
    "    \n",
    "# use widget interaction basic    \n",
    "widgets.interactive(result_visualize, class_name=widgets.Dropdown(\n",
    "    options=list(dict_results.keys()),\n",
    "    value=list(dict_results.keys())[0],\n",
    "    description='Class:',\n",
    "    disabled=False,\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         etag              label  \\\n",
      "0    a039cb37f290fe4a4127bbd2            holiday   \n",
      "0    a039cb37f290fe4a4127bbd2        gift giving   \n",
      "0    a039cb37f290fe4a4127bbd2     family moments   \n",
      "1    b6dbe02fdf00f08a61a44b8f            holiday   \n",
      "1    b6dbe02fdf00f08a61a44b8f        gift giving   \n",
      "..                        ...                ...   \n",
      "515  bf3731149d36b7af5703e1ea          halloween   \n",
      "516  d2e47869a53995007d279ac4          halloween   \n",
      "517  1a9528594a26c2645da8076a  none of the above   \n",
      "518  ac3eb9016c3b19c85006b0ac            holiday   \n",
      "519  5f0dc388e0964e590a69f166  none of the above   \n",
      "\n",
      "                                                                                url  \\\n",
      "0     https://vmlr-workshop.STORAGE/gifts/vid_gift_give_take_8-4-of-12.mp4   \n",
      "0     https://vmlr-workshop.STORAGE/gifts/vid_gift_give_take_8-4-of-12.mp4   \n",
      "0     https://vmlr-workshop.STORAGE/gifts/vid_gift_give_take_8-4-of-12.mp4   \n",
      "1    https://vmlr-workshop.STORAGE/gifts/vid_gift_give_take_28-7-of-16.mp4   \n",
      "1    https://vmlr-workshop.STORAGE/gifts/vid_gift_give_take_28-7-of-16.mp4   \n",
      "..                                                                              ...   \n",
      "515    https://vmlr-workshop.STORAGE/halloween/vid_halloween_6-6-of-34.mp4   \n",
      "516   https://vmlr-workshop.STORAGE/halloween/vid_halloween_9-49-of-57.mp4   \n",
      "517   https://vmlr-workshop.STORAGE/gifts/vid_gift_give_take_38-1-of-7.mp4   \n",
      "518             https://vmlr-workshop.STORAGE/xmas/vid_xmas_8-45-of-49.mp4   \n",
      "519             https://vmlr-workshop.STORAGE/xmas/vid_xmas_4-41-of-54.mp4   \n",
      "\n",
      "                                       asset  \n",
      "0     gifts/vid_gift_give_take_8-4-of-12.mp4  \n",
      "0     gifts/vid_gift_give_take_8-4-of-12.mp4  \n",
      "0     gifts/vid_gift_give_take_8-4-of-12.mp4  \n",
      "1    gifts/vid_gift_give_take_28-7-of-16.mp4  \n",
      "1    gifts/vid_gift_give_take_28-7-of-16.mp4  \n",
      "..                                       ...  \n",
      "515    halloween/vid_halloween_6-6-of-34.mp4  \n",
      "516   halloween/vid_halloween_9-49-of-57.mp4  \n",
      "517   gifts/vid_gift_give_take_38-1-of-7.mp4  \n",
      "518             xmas/vid_xmas_8-45-of-49.mp4  \n",
      "519             xmas/vid_xmas_4-41-of-54.mp4  \n",
      "\n",
      "[826 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Tokenizing and embedding tags...\")\n",
    "\n",
    "def list2vocab(nlp, list_tags, output_file):\n",
    "    \"\"\"Given a specific model, map the file set (single line of text) into an output embedding space\"\"\"\n",
    "    vocab = Vocab()\n",
    "    idx = 0\n",
    "    for tag_raw in list_tags:   # for each tag input\n",
    "        tag_raw = tag_raw.lower()\n",
    "        tag_id = nlp.vocab.strings[tag_raw]\n",
    "        if tag_id not in nlp.vocab:   # search existing one\n",
    "            tag_doc = nlp(tag_raw)\n",
    "            vocab.set_vector(tag_raw, tag_doc.vector)\n",
    "        else:\n",
    "            vocab.set_vector(tag_raw, nlp.vocab[tag_id].vector)\n",
    "        idx += 1\n",
    "        if (idx % 5000) == 0:\n",
    "            print(f\"list2vec: [{idx}/{len(list_tags)}] processing complete\")\n",
    "    # write a w2v file\n",
    "    path_target = Path(output_file).resolve()\n",
    "    vocab.to_disk(str(path_target))\n",
    "    return vocab\n",
    "\n",
    "AGG_TAG_EMBEDDING\n",
    "\n",
    "# keys, rows, score = nlp.vocab.vectors.most_similar(query_vect, batch_size=2048, n=5)\n",
    "# print(keys, rows, score)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# This block only needs to be run once.  If your notebook is inactive  #\n",
    "#     for an extended period of time and restarts, then you may have   #\n",
    "#     to re-run this again to reinstall vertica library.               #\n",
    "########################################################################\n",
    "import os\n",
    "\n",
    "\n",
    "# Some commands to get thigns running\n",
    "!pip install contentai_metadata_flatten\n",
    "!pip install git+https://CODE_SITE/scm/st_lq/pylq.git\n",
    "\n",
    "# See custom pypi package creation here:\n",
    "# !pip install --extra-index-url https://repocentral.it.DOMAIN:8443/nexus/repository/att-pypi/ contentai_metadata_flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in os.environ:\n",
    "    if \"user\" in k.lower() or \"att\" in k.lower() :\n",
    "        print(k, os.environ[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
