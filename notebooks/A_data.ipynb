{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning Workshop: Content Insights 2020](assets/mlci_banner.jpg)\n",
    "\n",
    "# Machine Learning Workshop: Content Insights 2020\n",
    "\n",
    "Welcome to the workshop notebooks!  These notebooks are designed to give you a walk through the steps of creating a model, refining it with user labels, and testing it on content.  You can access the main [workshop forum page](https://INFO_SITE/forums/html/forum?id=241a0b77-7aa6-4fef-9f25-5ea351825725&ps=25), the [workshop files repo](https://INFO_SITE/communities/service/html/communityview?communityUuid=fb400868-b17c-44d8-8b63-b445d26a0be4#fullpageWidgetId=W403a0d6f86de_45aa_8b67_c52cf90fca16&folder=d8138bef-9182-4bdc-8b12-3c88158a219c), or the [symposium home page](https://software.web.DOMAIN) for additional help.\n",
    "\n",
    "The notebooks are divided into five core components: (A) setup & data, (B) model exploration, (C) labeling, (D) active labeling, (E) and deployment.  You are currently viewing the *setup & data* workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for running the workshop; we'll repeat these in the top line of each workbook.\n",
    "#   why repeat them? the backup routine only serializes .ipynb files, so others will need \n",
    "#   to be downloaded again if your compute instance restarts (a small price to pay, right?)\n",
    "\n",
    "WORKSHOP_BASE = \"https://vmlr-workshop.STORAGE\"\n",
    "# WORKSHOP_BASE = \"http://content.research.DOMAIN/projects/mlci_2020\"\n",
    "AGG_METADATA = \"agg_metadata.pkl.gz\"\n",
    "AGG_LABELS = \"agg_labels.pkl.gz\"\n",
    "CLASS_LABELS = \"assets/classes.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Dependency Downloads\n",
    "\n",
    "This section will grab and install other package files required for the \n",
    "execution of this workshop.  This may be required if you did not start from the \n",
    "all-in-one package download.  \n",
    "\n",
    "* `packages` - contains installed packages that may not exist in other public repos\n",
    "* `data` - contains the data that will be used in this workshop in [pickled](https://docs.python.org/3/library/pickle.html) and [hdf5](https://docs.h5py.org/en/stable/) file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quinone/Documents/projects/miracle/ml_hack2020/cmlp/work/packages/lq-latest-py3-none-any.whl already exists!\n",
      "/Users/quinone/Documents/projects/miracle/ml_hack2020/cmlp/work/packages/features_tag.tgz already exists!\n",
      "/Users/quinone/Documents/projects/miracle/ml_hack2020/cmlp/work/packages/features_binary.tgz already exists!\n",
      "... file download complete.\n",
      "Installing packages...\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n",
      "Expanding features...\n",
      "...all setup operations complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ATT_JUPYTER = False\n",
    "for k in os.environ:   # scan some environment vars\n",
    "    if \"user\" in k.lower():   # found user, check setting\n",
    "        if \"DOMAIN\" in os.environ[k].lower():\n",
    "            ATT_JUPYTER = True   # found AT&T, set marker\n",
    "\n",
    "proxies = None\n",
    "if ATT_JUPYTER:   # switch for proxy setting\n",
    "    # os.environ['http_proxy'] = 'http://PROXY:8080'\n",
    "    # os.environ['https_proxy'] = 'http://PROXY:8080'\n",
    "    os.environ['no_proxy'] = '*.DOMAIN'\n",
    "    proxies = {\n",
    "        \"http\": \"http://pxyapp.proxy.DOMAIN:8080\",\n",
    "        \"https\": \"http://pxyapp.proxy.DOMAIN:8080\",\n",
    "    }\n",
    "    os.environ['http_proxy'] = proxies['http']\n",
    "    os.environ['https_proxy'] = proxies['https']\n",
    "\n",
    "files = {\n",
    "    \"lq-latest-py3-none-any.whl\": f\"{WORKSHOP_BASE}/packages/lq-latest-py3-none-any.whl\"\n",
    "    , \"features_tag.tgz\": f\"{WORKSHOP_BASE}/packages/features_tag.tgz\"\n",
    "    , \"features_binary.tgz\": f\"{WORKSHOP_BASE}/packages/features_binary.tgz\"\n",
    "}\n",
    "\n",
    "def remote_download(dict_files, proxies, dir_dest=\"packages\", overwrite=False):\n",
    "    import requests\n",
    "\n",
    "    path_dest = Path(dir_dest)\n",
    "    if not path_dest.exists():\n",
    "        path_dest.mkdir(parents=True)\n",
    "\n",
    "    for name, location in files.items():\n",
    "        path_local = path_dest.joinpath(name)\n",
    "        if path_local.exists() and not overwrite:\n",
    "            print(f\"{str(path_local.resolve())} already exists!\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Getting file '{location}'\")\n",
    "        r = requests.get(location, proxies=proxies, stream=True)\n",
    "        print(f\"Writing to file {name}\")\n",
    "        with path_local.open('wb') as f:\n",
    "            for chunk in r.iter_content(4096):\n",
    "                f.write(chunk)\n",
    "\n",
    "# consider changing this to True if you have odd install errors\n",
    "remote_download(files, overwrite=False, proxies=proxies)   \n",
    "print(\"... file download complete.\")\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "\n",
    "# the labelquest client library, this is mostly used in workbook 'B'\n",
    "!pip install -q --no-cache-dir --no-index packages/lq-latest-py3-none-any.whl\n",
    "\n",
    "# some visualization helpers for the workshop\n",
    "!pip install -q --no-cache-dir --no-index ipywidgets\n",
    "\n",
    "# include basic text mapping utility\n",
    "!pip install -q --no-cache-dir --no-index spacy\n",
    "\n",
    "# check out this URL for other text models, but since we're not using it much, a smaller version is okay \n",
    "#    https://spacy.io/models/en\n",
    "!python -m spacy download en_core_web_md -q --no-cache-dir --no-index\n",
    "\n",
    "print(\"Expanding features...\")\n",
    "!cd packages && ls *.tgz | xargs -I {} tar -zxf {}\n",
    "\n",
    "print(\"...all setup operations complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Textual Features and Tags\n",
    "\n",
    "In this section, we'll take our first look at the timed metadata. Specifically, this data has been computed within the [ContentAI](https://www.contentai.io/) platform and downloaded with the steps above.  ContentAI is a flexible cloud-native platform that can accept a content reference and run one or more [extractors](https://www.contentai.io/docs/extractors) to provide metadata, processed video, etc.  \n",
    "\n",
    "In this workshop, we'll be looking at some of the tags and recognition features that come from the [Azure extrator](https://www.contentai.io/docs/azure-videoindexer-api) which wraps many of the features from the [Azure Video Indexer](https://azure.microsoft.com/en-us/services/media-services/video-indexer/) service in a secure fashion.  \n",
    "```\n",
    "content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer\n",
    "content/vmlr-workshop/gifts/vid_gift_give_take_9-2-of-14.mp4/batches/1hl1l0V3BNumdsZc3DaAJd3JlB2/azure_videoindexer\n",
    "content/vmlr-workshop/xmas/vid_xmas_8-28-of-49.mp4/batches/1hiPieI6mb2Dyzzsg84TCI5cCmM/azure_videoindexer\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-34-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-19-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer\n",
    "```\n",
    "\n",
    "Further, we'll also be looking at a normalized (or flattened) version of the data produced by the [DSAI Metadata Flattener](https://www.contentai.io/docs/dsai_metadata_flatten) ([code repo](https://CODE_SITE/projects/ST_VMLR/repos/contentai-metadata-flatten/browse)) which has been rendered to CSVs.  \n",
    "```\n",
    "content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/gifts/vid_gift_give_take_9-2-of-14.mp4/batches/1hl1l0V3BNumdsZc3DaAJd3JlB2/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/xmas/vid_xmas_8-28-of-49.mp4/batches/1hiPieI6mb2Dyzzsg84TCI5cCmM/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-34-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "content/vmlr-workshop/halloween/vid_halloween_7-19-of-56.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Insights\n",
    "As an example, let's parse and store the flattened data for Azure output, activity output, and moderation output from the flattener service.\n",
    "\n",
    "\n",
    "For inquisitive minds, the original data from the extractors is also include, typically as a simple `data.json` in their corresponding diectory.  If you've got the hang of it, try to figure out what other extractors have been run for this asset.\n",
    "\n",
    "\n",
    "```\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_videocnn/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_videocnn/data.hdf5\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_activity_classifier/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_dsai_activity_classifier.csv.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_dsai_moderation.csv.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/wbTimeTaggedMetadata.json.gz\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_vggish/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_vggish/data.hdf5\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/dsai_moderation_image/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.csv\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.json\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.ttml\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.txt\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.vtt\n",
    ".../1hhadDBuEtRUPd6v8vCr5H3346r/azure_videoindexer/data.srt\n",
    "```\n",
    "\n",
    "(answer for above...)\n",
    "* **input path** - `content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4`\n",
    "* **nested job id** - `batches/1hhadDBuEtRUPd6v8vCr5H3346r`\n",
    "* **extractor and data file** - `dsai_videocnn/data.json`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping re-create of metadata file 'agg_metadata.pkl.gz'...\n",
      "New columns in this data... ['time_begin', 'source_event', 'tag_type', 'time_end', 'time_event', 'tag', 'score', 'details', 'extractor', 'asset']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path_metadata = Path(AGG_METADATA)\n",
    "if path_metadata.exists():\n",
    "    print(f\"Skipping re-create of metadata file '{str(path_metadata)}'...\")\n",
    "    df_flatten = pd.read_pickle(str(path_metadata))\n",
    "else:\n",
    "    df_flatten = None\n",
    "    num_files = 0\n",
    "    path_content = Path(\"packages/content/vmlr-workshop\")\n",
    "    list_files = list(path_content.rglob(\"csv_flatten*.csv*\"))\n",
    "    print(f\"Ingesting {len(list_files)} flatten files in path '{str(path_content)}'...\")\n",
    "    for path_file in list_files:  # search for flattened files\n",
    "        df_new = pd.read_csv(path_file)\n",
    "        # FROM content/vmlr-workshop/halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten/csv_flatten_azure_videoindexer.csv.gz -> \n",
    "        # TO halloween/vid_halloween_0-13-of-23.mp4/batches/1hhadDBuEtRUPd6v8vCr5H3346r/dsai_metadata_flatten (relative_to)\n",
    "        # TO halloween/vid_halloween_0-13-of-23.mp4  (joining base path parts)\n",
    "        path_asset = Path(*path_file.parent.relative_to(path_content).parts[:2])\n",
    "        df_new['tag'] = df_new['tag'].str.lower()   # lower case the tags\n",
    "        df_new['details'] = df_new['details'].str.lower()   # lower case the enhanced information\n",
    "        df_new['asset'] = str(path_asset)\n",
    "        if df_flatten is None:   # first one we saw\n",
    "            df_flatten = df_new\n",
    "        else:\n",
    "            df_flatten = df_flatten.append(df_new, ignore_index=True)   # append new dataframe\n",
    "        num_files += 1\n",
    "        if num_files % 500 == 0:\n",
    "            print(f\"... read {num_files}...\")\n",
    "    df_flatten.reset_index(drop=True, inplace=True)  # drop prior index\n",
    "    df_flatten.to_pickle(str(path_metadata))\n",
    "    print(f\"Wrote {num_files} aggregations to file '{str(path_metadata)}'...\")\n",
    "\n",
    "print(f\"New columns in this data... {list(df_flatten.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting tags\n",
    "Let's plot some statistics about tags, both their numbers and their names.  First, a histogram of how many unique and total tags were present for an asset.  This plot helps us find average number of tags, both in raw counts and unique tags for an asset.  Second, an average and raw count of the top `N` tags found from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820389921032491bbe6664ae967959d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatRangeSlider(value=(0.5, 1.0), continuous_update=False, description='Score Range:', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.tag_count_hist(x)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# this is a handy update function\n",
    "def tag_count_hist(x):\n",
    "    x = (round(x[0], 2), round(x[1], 2))\n",
    "    df_sub = df_flatten[(df_flatten['score'] >= x[0]) & (df_flatten['score'] <= x[1])]\n",
    "    df_pairs = df_sub.groupby(['asset','tag']).count()['score'].reset_index()   # group by two params, reset into dataframe\n",
    "    df_unitags = df_pairs.groupby(['asset'])['score'].agg(['count','sum']).reset_index()   # group by asset to find unique tag count per asset\n",
    "    df_unitags.rename(columns={\"count\":\"unique tags\", \"sum\":\"total tags\"}, inplace=True)\n",
    "    # print(df_unitags)\n",
    "    ax = df_unitags.plot.hist(by='asset', bins=40, figsize=(10,4), alpha=0.75)\n",
    "    pl.title(f\"Histogram of Unique Tags Among Assets ({x[0]} >= Score >= {x[1]})\")\n",
    "    pl.ylabel('number of assets')\n",
    "    pl.xlabel('count of tags')\n",
    "    pl.grid()\n",
    "    pl.show()\n",
    "    \n",
    "    df_pairs = df_sub.groupby(['tag','asset']).count()['score'].reset_index()   # group by two params, reset into dataframe\n",
    "    df_unitags = df_pairs.groupby(['tag'])['score'].agg(['count','sum']).reset_index()   # group by asset to find unique tag count per asset\n",
    "    df_unitags.sort_values('sum', ignore_index=True, inplace=True, ascending=False)\n",
    "    df_unitags.rename(columns={\"count\":\"Asset Frequency\", \"sum\":\"Total Frequency\"}, inplace=True)\n",
    "    top_n = 20\n",
    "    df_topn = df_unitags.iloc[:top_n]\n",
    "    ax = df_topn.plot.barh(x='tag', figsize=(10,4), width=0.8, log=True)\n",
    "    pl.title(f\"Total and Asset Frequency of {top_n} Most Frequent Tags ({x[0]} >= Score >= {x[1]})\")\n",
    "    pl.ylabel('tag text')\n",
    "    pl.xlabel('count of tags')\n",
    "    pl.grid()\n",
    "    pl.show()\n",
    "    \n",
    "    \n",
    "\n",
    "# get an interactive widget/graph\n",
    "widgets.interact(tag_count_hist, x=widgets.FloatRangeSlider(\n",
    "    value=[0.5, 1.0],\n",
    "    step=0.05,\n",
    "    min=df_flatten['score'].min(),\n",
    "    max=df_flatten['score'].max(),\n",
    "    description='Score Range:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Ads\n",
    "\n",
    "One goal of combining timed metadata and content is for better alignment of content from WarnerMedia and ads from Xandr creatives.  The iamge below demonstrates one example where a detected keyword or scene can trigger an ad that is realted.  Without this technology, this ad spot (or inventory) may be undersold and filled with an unrelated or standard campaign ad.\n",
    "\n",
    "![Contextual Ad Product](assets/mlci_contextual.jpg)\n",
    "\n",
    "## Class Exploration\n",
    "Let's quickly load and display the target classes used in this experiment. The table below indicates the class and the definition utilized for our classifier.  The field `primary` indicates whether or not a class will be used for performance evaluations.  Some non-primary classes were also included for additional experimentation, but they will not be the focus here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>definition</th>\n",
       "      <th>primary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>holiday</td>\n",
       "      <td>holiday scenes or objects like decorated trees, presents, or character, holiday party</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>halloween</td>\n",
       "      <td>halloween scenes where one or more characters are in costume, ideally one or more characters are trick-or-treating</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gift giving</td>\n",
       "      <td>scenes of gift giving, receiving, or opening/unwrapping</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>family moments</td>\n",
       "      <td>at least two people on screen, typically familes at parties, enjoying a meal, lounging at home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shopping scenes</td>\n",
       "      <td>one or more primary actors in a store-like environment; not necessary to see their face</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class  \\\n",
       "0          holiday   \n",
       "1        halloween   \n",
       "2      gift giving   \n",
       "3   family moments   \n",
       "4  shopping scenes   \n",
       "\n",
       "                                                                                                           definition  \\\n",
       "0                               holiday scenes or objects like decorated trees, presents, or character, holiday party   \n",
       "1  halloween scenes where one or more characters are in costume, ideally one or more characters are trick-or-treating   \n",
       "2                                                             scenes of gift giving, receiving, or opening/unwrapping   \n",
       "3                      at least two people on screen, typically familes at parties, enjoying a meal, lounging at home   \n",
       "4                             one or more primary actors in a store-like environment; not necessary to see their face   \n",
       "\n",
       "   primary  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "with open(CLASS_LABELS, 'r') as f:\n",
    "    obj_classes = json.load(f)\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "df_classes = pd.read_json(CLASS_LABELS)\n",
    "df_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Result Browsing\n",
    "Below, we've created a simple method to view keyframe results from the video assets used in this workshop.  It uses simple widgets (as we did above) and displays them in a columnar form in ths notebook.  In addition to numerical performance (accuracy, AUC, etc.), we can visually inspect the performance of a classifier model with this utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934e2315e0114acc9d5e4e04993105a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Results demo</h3>'), GridBox(children=(HTML(value=\"\\n        <a href='https://vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import random\n",
    "\n",
    "def path_to_image_html(path_result, score, base=WORKSHOP_BASE):\n",
    "    '''\n",
    "     This function essentially convert the image url to \n",
    "     '<img src=\"'+ path + '\"/>' format. And one can put any\n",
    "     formatting adjustments to control the height, aspect ratio, size etc.\n",
    "     within as in the below example. \n",
    "    '''\n",
    "    path_parts = path_result.split('/')\n",
    "    url_full = f\"{base}/content/{path_parts[0]}/keyframe/{path_parts[1]}.jpg\"\n",
    "    return f\"\"\"\n",
    "        <a href='{url_full}' title='{path_result}' target='_new'><img width='100%' src='{url_full}' /></a>\n",
    "        <small>{round(score, 4)}</small>\n",
    "        \"\"\"\n",
    "\n",
    "def display_results(list_results, max_results=32, num_cols=8, base=WORKSHOP_BASE, title=\"Example Results\"):\n",
    "    \"\"\"Input a list of tuples `[(path,score), ...]` and show those results in columnar format\"\"\"\n",
    "    width_col = f\"{round(100/num_cols)}%\"\n",
    "    list_results.sort(reverse=True, key=lambda x: x[-1]) \n",
    "    list_html = [widgets.HTML(path_to_image_html(*x, base=base)) for x in list_results]\n",
    "    return widgets.VBox([widgets.HTML(f\"<h3>{title}</h3>\"), widgets.GridBox(list_html,\n",
    "               layout=widgets.Layout(grid_template_columns=f\"repeat(8, {width_col})\"))])\n",
    "\n",
    "# demo of input as a single result, but with random scores\n",
    "display_results([[\"xmas/vid_xmas_9-62-of-67.mp4\", random.uniform(0,1)] for i in range(16)], title=\"Results demo\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier 1: Classifier By Text\n",
    "Our first classifier model will be constructed by using classical mapping of the class definitions through text search and mapping to tag names.  Using the classes loaded above, let's perform a quick mapping.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, class                                                                                       holiday\n",
      "definition    holiday scenes or objects like decorated trees, presents, or character, holiday party\n",
      "primary                                                                                           1\n",
      "Name: 0, dtype: object)\n",
      "(1, class                                                                                                                  halloween\n",
      "definition    halloween scenes where one or more characters are in costume, ideally one or more characters are trick-or-treating\n",
      "primary                                                                                                                        1\n",
      "Name: 1, dtype: object)\n",
      "(2, class                                                     gift giving\n",
      "definition    scenes of gift giving, receiving, or opening/unwrapping\n",
      "primary                                                             1\n",
      "Name: 2, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "for row in df_classes[df_classes['primary']==1].iterrows():  # iterate rows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# This block only needs to be run once.  If your notebook is inactive  #\n",
    "#     for an extended period of time and restarts, then you may have   #\n",
    "#     to re-run this again to reinstall vertica library.               #\n",
    "########################################################################\n",
    "import os\n",
    "\n",
    "\n",
    "# Some commands to get thigns running\n",
    "!pip install contentai_metadata_flatten\n",
    "!pip install git+https://CODE_SITE/scm/st_lq/pylq.git\n",
    "\n",
    "# See custom pypi package creation here:\n",
    "# !pip install --extra-index-url https://repocentral.it.DOMAIN:8443/nexus/repository/att-pypi/ contentai_metadata_flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in os.environ:\n",
    "    if \"user\" in k.lower() or \"att\" in k.lower() :\n",
    "        print(k, os.environ[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
