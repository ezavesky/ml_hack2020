{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning Workshop: Content Insights 2020](../assets/mlci_banner.jpg)\n",
    "\n",
    "# Machine Learning Workshop: Content Insights 2020\n",
    "\n",
    "Welcome to the workshop notebooks!  These notebooks are designed to give you a walk through the steps of creating a model, refining it with user labels, and testing it on content.  You can access the main [workshop forum page](https://INFO_SITE/forums/html/forum?id=241a0b77-7aa6-4fef-9f25-5ea351825725&ps=25), the [workshop files repo](https://INFO_SITE/communities/service/html/communityview?communityUuid=fb400868-b17c-44d8-8b63-b445d26a0be4#fullpageWidgetId=W403a0d6f86de_45aa_8b67_c52cf90fca16&folder=d8138bef-9182-4bdc-8b12-3c88158a219c), or the [symposium home page](https://software.web.DOMAIN) for additional help.\n",
    "\n",
    "The notebooks are divided into five core components: (A) setup & data, (B) model exploration, (C) labeling, (D) active labeling, (E) and deployment.  You are currently viewing the *setup & data* workbook.\n",
    "\n",
    "Start your LabelQuest journey by clicking here:\n",
    "\n",
    "## Log In: https://APP_SITE\n",
    "\n",
    "## Get your Token : https://APP_SITE/api/lq/v1/uam/auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for running the workshop; we'll repeat these in the top line of each workbook.\n",
    "#   why repeat them? the backup routine only serializes .ipynb files, so others will need \n",
    "#   to be downloaded again if your compute instance restarts (a small price to pay, right?)\n",
    "\n",
    "WORKSHOP_BASE = \"https://vmlr-workshop.STORAGE\"\n",
    "# WORKSHOP_BASE = \"http://content.research.DOMAIN/projects/mlci_2020\"\n",
    "AGG_METADATA = \"models/agg_metadata.pkl.gz\"            # custom file for merged metadata\n",
    "\n",
    "# you need to provide this (copy the string from https://APP_SITE/api/lq/v1/uam/auth)\n",
    "LQ_JWT = \"\"  \n",
    "LQ_ROOT_URL = \"https://APP_SITE\"\n",
    "LQ_ROOT_SSL_VERIFY = False\n",
    "\n",
    "IMDB5000_FEAT = \"packages/movie_metadata.csv\"   # public dataset for movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonic API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "\n        No token detected (in LQ_JWT), please authenticate and get your JWT token.\n        1. Log into the test instance of LQ - https://APP_SITE/\n        2a. Get your LQ token from here - https://APP_SITE/api/lq/v1/uam/auth\n        2b. OR Save the produced JSON file to the same directory as this script (as auth.json)\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a15b67795946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mGet\u001b[0m \u001b[0myour\u001b[0m \u001b[0mLQ\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhere\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mlq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlq\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0muam\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOR\u001b[0m \u001b[0mSave\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproduced\u001b[0m \u001b[0mJSON\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mscript\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mas\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \"\"\")\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: \n        No token detected (in LQ_JWT), please authenticate and get your JWT token.\n        1. Log into the test instance of LQ - https://APP_SITE/\n        2a. Get your LQ token from here - https://APP_SITE/api/lq/v1/uam/auth\n        2b. OR Save the produced JSON file to the same directory as this script (as auth.json)\n    "
     ]
    }
   ],
   "source": [
    "import lq\n",
    "from lq.content_label import ContentLabeler\n",
    "\n",
    "if not LQ_JWT:\n",
    "    LQ_JWT = ContentLabeler.jwt_load(\"auth.json\")\n",
    "if not LQ_JWT:\n",
    "    raise Exception(\"\"\"\n",
    "        No token detected (in LQ_JWT), please authenticate and get your JWT token.\n",
    "        1. Log into the test instance of LQ - https://APP_SITE/\n",
    "        2a. Get your LQ token from here - https://APP_SITE/api/lq/v1/uam/auth\n",
    "        2b. OR Save the produced JSON file to the same directory as this script (as auth.json)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation by Association\n",
    "In our user survey, the majority indicated they'd be willing to provide a few labels to a service for better product recommendations, so let's test that promise and its efficacy.  \n",
    "\n",
    "![willingness to label](../assets/labelquest_agreement.jpg)\n",
    "\n",
    "In the remainder of this notebook, we'll use the [IMDB 5000 dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset) that has a small collection of movies and some metadata about actors and directors.  Unfortunately, you won't be able to make direct requests to WarnerBrothers or HBO to produce your most preferred combination, but this example should demonstrate the power of rapid model refinement with just a few labels.\n",
    "\n",
    "To jump right to the fun part, we've done a little bit of preprocessing to formulate a few models on the data.  The code in the following cell **will not be executed** because we did it for you, but it's provided here for some fun examples.\n",
    "\n",
    "1. **Actor Affinity** - finding your preferred actor by the links to others\n",
    "2. **Genre Preferences** - finding preference for genres by direct categorical links\n",
    "3. **Crowd Alignment** - how closely do your opinions match that of others, as determined by `likes` or `box office gross`\n",
    "4. **Embedded Topics** - (advanced) a method that uses our NLP embedding to get recommendations, similar to genre work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n",
      "       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n",
      "       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n",
      "       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n",
      "       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n",
      "       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n",
      "       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n",
      "       'imdb_score', 'aspect_ratio', 'movie_facebook_likes'],\n",
      "      dtype='object') 5043\n",
      "Index(['color', 'director_name', 'num_critic_for_reviews', 'duration',\n",
      "       'director_facebook_likes', 'actor_3_facebook_likes', 'actor_2_name',\n",
      "       'actor_1_facebook_likes', 'gross', 'genres', 'actor_1_name',\n",
      "       'movie_title', 'num_voted_users', 'cast_total_facebook_likes',\n",
      "       'actor_3_name', 'facenumber_in_poster', 'plot_keywords',\n",
      "       'movie_imdb_link', 'num_user_for_reviews', 'language', 'country',\n",
      "       'content_rating', 'budget', 'title_year', 'actor_2_facebook_likes',\n",
      "       'imdb_score', 'aspect_ratio', 'movie_facebook_likes', 'people'],\n",
      "      dtype='object') 58016\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>director_name</th>\n",
       "      <th>num_critic_for_reviews</th>\n",
       "      <th>duration</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>actor_2_name</th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>gross</th>\n",
       "      <th>genres</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>budget</th>\n",
       "      <th>title_year</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>movie_facebook_likes</th>\n",
       "      <th>people</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Color</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>723.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>Action</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.78</td>\n",
       "      <td>33000</td>\n",
       "      <td>James Cameron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Color</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>723.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.78</td>\n",
       "      <td>33000</td>\n",
       "      <td>James Cameron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Color</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>723.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>Fantasy</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.78</td>\n",
       "      <td>33000</td>\n",
       "      <td>James Cameron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Color</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>723.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.78</td>\n",
       "      <td>33000</td>\n",
       "      <td>James Cameron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Color</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>723.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>Action</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.78</td>\n",
       "      <td>33000</td>\n",
       "      <td>CCH Pounder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   color  director_name  num_critic_for_reviews  duration  \\\n",
       "0  Color  James Cameron                   723.0     178.0   \n",
       "0  Color  James Cameron                   723.0     178.0   \n",
       "0  Color  James Cameron                   723.0     178.0   \n",
       "0  Color  James Cameron                   723.0     178.0   \n",
       "0  Color  James Cameron                   723.0     178.0   \n",
       "\n",
       "   director_facebook_likes  actor_3_facebook_likes      actor_2_name  \\\n",
       "0                      0.0                   855.0  Joel David Moore   \n",
       "0                      0.0                   855.0  Joel David Moore   \n",
       "0                      0.0                   855.0  Joel David Moore   \n",
       "0                      0.0                   855.0  Joel David Moore   \n",
       "0                      0.0                   855.0  Joel David Moore   \n",
       "\n",
       "   actor_1_facebook_likes        gross     genres  ... language country  \\\n",
       "0                  1000.0  760505847.0     Action  ...  English     USA   \n",
       "0                  1000.0  760505847.0  Adventure  ...  English     USA   \n",
       "0                  1000.0  760505847.0    Fantasy  ...  English     USA   \n",
       "0                  1000.0  760505847.0     Sci-Fi  ...  English     USA   \n",
       "0                  1000.0  760505847.0     Action  ...  English     USA   \n",
       "\n",
       "   content_rating       budget title_year  actor_2_facebook_likes imdb_score  \\\n",
       "0           PG-13  237000000.0     2009.0                   936.0        7.9   \n",
       "0           PG-13  237000000.0     2009.0                   936.0        7.9   \n",
       "0           PG-13  237000000.0     2009.0                   936.0        7.9   \n",
       "0           PG-13  237000000.0     2009.0                   936.0        7.9   \n",
       "0           PG-13  237000000.0     2009.0                   936.0        7.9   \n",
       "\n",
       "  aspect_ratio  movie_facebook_likes         people  \n",
       "0         1.78                 33000  James Cameron  \n",
       "0         1.78                 33000  James Cameron  \n",
       "0         1.78                 33000  James Cameron  \n",
       "0         1.78                 33000  James Cameron  \n",
       "0         1.78                 33000    CCH Pounder  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMDB5000_FEAT - train some simple models according to above?\n",
    "df_movies = pd.read_csv(IMDB5000_FEAT)\n",
    "print(df_movies.columns, len(df_movies))\n",
    "\n",
    "# task 1: encode all features into movie-based rows\n",
    "# task 2: select actors by affinity to likes \n",
    "\n",
    "\n",
    "\n",
    "df_movies['people'] = df_movies['director_name'].map(str) + \"|\" + df_movies['actor_1_name'].map(str) + \\\n",
    "                         \"|\" + df_movies['actor_2_name'].map(str) + \"|\" + df_movies['actor_3_name'].map(str)\n",
    "df_movies['people'] = df_movies['people'].apply(lambda x: x.split('|'))\n",
    "df_movies['genres'] = df_movies['genres'].apply(lambda x: x.split('|'))\n",
    "df_movies = df_movies.explode('people').explode('genres')\n",
    "\n",
    "print(df_movies.columns, len(df_movies))\n",
    "df_movies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP model ready to go!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# OLD CODE, but examples of thigns that would be run FOR the user to be used in above experiments\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from scipy import spatial\n",
    "from pathlib import Path\n",
    "\n",
    "def text2doc(nlp, tag_raw):\n",
    "    \"\"\"Given a raw text input, tokenize it to remove stop words\"\"\"\n",
    "    return [x for x in nlp(tag_raw) if not x.is_stop and not x.is_punct]\n",
    "\n",
    "def doc2vec(nlp, doc, target_domain=None):\n",
    "    \"\"\"Given a specific model, clean line of text into an output embedding space\"\"\"\n",
    "    # https://spacy.io/usage/vectors-similarity\n",
    "    if target_domain is None:\n",
    "        target_domain = nlp.vocab\n",
    "    if type(doc) != list:\n",
    "        doc = [doc]\n",
    "    tag_doc = None\n",
    "    for token in doc:\n",
    "        tag_id = target_domain.strings[token.text]\n",
    "        if tag_id in target_domain.vectors:   # search existing one\n",
    "            new_vec = target_domain.vectors[tag_id]\n",
    "        elif type(token)==str:\n",
    "            new_vec = nlp(token).vector\n",
    "        else:\n",
    "            new_vec = token.vector\n",
    "        if tag_doc is None:\n",
    "            tag_doc = new_vec\n",
    "        else:\n",
    "            tag_doc += new_vec\n",
    "    return tag_doc\n",
    "\n",
    "# doc = text2doc(nlp, \"this is a phrase to clean\")\n",
    "# vec = doc2vec(nlp, doc)\n",
    "\n",
    "# also load our spacy NLP model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "print(\"NLP model ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and embedding classes...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f86d75a03b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print([x for x in df_classes[df_classes['primary']==1].iloc])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokenizing and embedding classes...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlist_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlist_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# iterate row indexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_classes' is not defined"
     ]
    }
   ],
   "source": [
    "# let's plow through our class dataframe and do the cleaning and mapping\n",
    "# print([x for x in df_classes[df_classes['primary']==1].iloc])\n",
    "print(\"Tokenizing and embedding classes...\")\n",
    "list_tokens = [\"\"] * len(df_classes)\n",
    "list_vect = [np.ndarray((1, nlp.vocab.vectors.shape[1]))] * len(df_classes)\n",
    "for idx in range(len(df_classes)):  # iterate row indexes\n",
    "    row = df_classes.iloc[idx]\n",
    "    # tokenize to remove tags, return tokens\n",
    "    doc = text2doc(nlp, row[\"definition\"]) + text2doc(nlp, row[\"class\"])\n",
    "    list_tokens[idx] = [str(x) for x in doc]\n",
    "    # convert to an embedding array\n",
    "    list_vect[idx] = doc2vec(nlp, doc)\n",
    "df_classes[\"token\"] = list_tokens\n",
    "df_classes[\"embedding\"] = list_vect\n",
    "\n",
    "# now collect all of the tags that match simple text bag\n",
    "print(\"Lookup specific tags by match...\")\n",
    "list_tags = df_flatten['tag'].unique()\n",
    "map_tokens = {}\n",
    "embed_tokens = np.ndarray((len(list_tags), nlp.vocab.vectors.shape[1]))\n",
    "for idx in range(len(list_tags)):   # iterate through full list of known tags\n",
    "    doc = text2doc(nlp, list_tags[idx])\n",
    "    for x in doc:  # create map/reference to each term\n",
    "        x = str(x)\n",
    "        if x not in map_tokens:  # first time to see this tag?\n",
    "            map_tokens[x] = []\n",
    "        map_tokens[x].append(idx)   # save reference to original tag set\n",
    "    embed_tokens[idx, :] = doc2vec(nlp, doc)  # compute embedding \n",
    "\n",
    "# df_classes[\"token\"] = list_tokens\n",
    "# df_classes[\"embedding\"] = list_vect\n",
    "print(f\"Count of new text-based mapping: {len(map_tokens)}...\")\n",
    "print(f\"Shape of embedded tag matrix: {embed_tokens.shape}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
